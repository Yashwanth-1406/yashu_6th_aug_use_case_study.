{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize Spark Scala Env."
      ],
      "metadata": {
        "id": "c6Z58K9N1_2E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "78Oxr-PY1pA6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b8a294f-0027-49c4-fbff-c7351753fe0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive to store cached files\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set variables\n",
        "strBasePath=\"/content/drive/MyDrive/IBM-DE-Spark-Scala\"\n",
        "scala_deb_path = strBasePath+\"/scala-2.12.18.deb\"\n",
        "spark_tgz_path = strBasePath+\"/spark-3.4.1-bin-hadoop3.tgz\"\n",
        "\n",
        "!mkdir -p /content/tmp\n",
        "import os\n",
        "# Download Scala .deb if not cached\n",
        "if not os.path.exists(scala_deb_path):\n",
        "    !wget -O \"{scala_deb_path}\" https://github.com/scala/scala/releases/download/v2.12.18/scala-2.12.18.deb\n",
        "\n",
        "# Download Spark tgz if not cached\n",
        "if not os.path.exists(spark_tgz_path):\n",
        "    !wget -O \"{spark_tgz_path}\" https://archive.apache.org/dist/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz\n",
        "\n",
        "# Copy cached files to working dir\n",
        "!cp \"{scala_deb_path}\" /content/tmp/scala-2.12.18.deb\n",
        "!cp \"{spark_tgz_path}\" /content/tmp/spark-3.4.1-bin-hadoop3.tgz\n",
        "\n",
        "# Install Java if not already present\n",
        "!java -version || apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "\n",
        "# Install Scala\n",
        "!dpkg -i /content/tmp/scala-2.12.18.deb\n",
        "\n",
        "# Extract Spark\n",
        "!tar xf /content/tmp/spark-3.4.1-bin-hadoop3.tgz -C /content\n",
        "\n",
        "# Set environment variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.4.1-bin-hadoop3\"\n",
        "os.environ[\"PATH\"] += f\":{os.environ['SPARK_HOME']}/bin\"\n",
        "\n",
        "# Confirm installation\n",
        "!java -version\n",
        "!scala -version\n",
        "!scalac -version\n",
        "!echo \"Spark path: $SPARK_HOME\"\n",
        "!ls $SPARK_HOME"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "xyJiL9W-2lXu",
        "outputId": "beab3207-a3d6-41f6-eca0-5ed137d4d384"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk version \"11.0.28\" 2025-07-15\n",
            "OpenJDK Runtime Environment (build 11.0.28+6-post-Ubuntu-1ubuntu122.04.1)\n",
            "OpenJDK 64-Bit Server VM (build 11.0.28+6-post-Ubuntu-1ubuntu122.04.1, mixed mode, sharing)\n",
            "Selecting previously unselected package scala.\n",
            "(Reading database ... 126284 files and directories currently installed.)\n",
            "Preparing to unpack /content/tmp/scala-2.12.18.deb ...\n",
            "Unpacking scala (2.12.18-400) ...\n",
            "Setting up scala (2.12.18-400) ...\n",
            "Creating system group: scala\n",
            "Creating system user: scala in scala with scala daemon-user and shell /bin/false\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "openjdk version \"11.0.28\" 2025-07-15\n",
            "OpenJDK Runtime Environment (build 11.0.28+6-post-Ubuntu-1ubuntu122.04.1)\n",
            "OpenJDK 64-Bit Server VM (build 11.0.28+6-post-Ubuntu-1ubuntu122.04.1, mixed mode, sharing)\n",
            "Scala code runner version 2.12.18 -- Copyright 2002-2023, LAMP/EPFL and Lightbend, Inc.\n",
            "Scala compiler version 2.12.18 -- Copyright 2002-2023, LAMP/EPFL and Lightbend, Inc.\n",
            "Spark path: /content/spark-3.4.1-bin-hadoop3\n",
            "bin   data\tjars\t    LICENSE   NOTICE  R\t\t RELEASE  yarn\n",
            "conf  examples\tkubernetes  licenses  python  README.md  sbin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Hello World in JAVA"
      ],
      "metadata": {
        "id": "P86TPUCo6-B2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile CreateDataFrame.java\n",
        "import org.apache.spark.sql.Dataset;\n",
        "import org.apache.spark.sql.Row;\n",
        "import org.apache.spark.sql.SparkSession;\n",
        "import org.apache.spark.sql.Encoders;\n",
        "import java.util.Arrays;\n",
        "import java.util.List;\n",
        "\n",
        "public class CreateDataFrame {\n",
        "    public static void main(String[] args) {\n",
        "        SparkSession spark = SparkSession.builder()\n",
        "                .appName(\"CreateDataFrameExample\")\n",
        "                .master(\"local[*]\")\n",
        "                .getOrCreate();\n",
        "\n",
        "        // Sample data\n",
        "        List<String> data = Arrays.asList(\"Java\", \"Python\", \"Scala\");\n",
        "\n",
        "        // Create DataFrame from a list\n",
        "        Dataset<String> df = spark.createDataset(data, Encoders.STRING());\n",
        "\n",
        "        // Show the DataFrame content\n",
        "        df.show();\n",
        "\n",
        "        spark.stop();\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Px_aQP7Q28De",
        "outputId": "3e4065f2-ad98-4baa-d2fe-69d8d09772f5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting CreateDataFrame.java\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "spark_home = os.environ.get(\"SPARK_HOME\")\n",
        "!javac -cp \"$spark_home/jars/*\" CreateDataFrame.java"
      ],
      "metadata": {
        "id": "sIaNIoC56kLN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "spark_home = os.environ.get(\"SPARK_HOME\")\n",
        "!java -cp \"$spark_home/jars/*:.\" CreateDataFrame"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iS5Z3wOblVCw",
        "outputId": "982ceea5-a1a9-4886-b0a4-ef8eeeceed22"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "25/08/06 05:01:12 INFO SparkContext: Running Spark version 3.4.1\n",
            "25/08/06 05:01:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "25/08/06 05:01:13 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 05:01:13 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "25/08/06 05:01:13 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 05:01:13 INFO SparkContext: Submitted application: CreateDataFrameExample\n",
            "25/08/06 05:01:13 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "25/08/06 05:01:13 INFO ResourceProfile: Limiting resource is cpu\n",
            "25/08/06 05:01:13 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "25/08/06 05:01:13 INFO SecurityManager: Changing view acls to: root\n",
            "25/08/06 05:01:13 INFO SecurityManager: Changing modify acls to: root\n",
            "25/08/06 05:01:13 INFO SecurityManager: Changing view acls groups to: \n",
            "25/08/06 05:01:13 INFO SecurityManager: Changing modify acls groups to: \n",
            "25/08/06 05:01:13 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "25/08/06 05:01:13 INFO Utils: Successfully started service 'sparkDriver' on port 41667.\n",
            "25/08/06 05:01:13 INFO SparkEnv: Registering MapOutputTracker\n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/content/spark-3.4.1-bin-hadoop3/jars/spark-unsafe_2.12-3.4.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "25/08/06 05:01:13 INFO SparkEnv: Registering BlockManagerMaster\n",
            "25/08/06 05:01:13 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "25/08/06 05:01:13 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "25/08/06 05:01:13 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "25/08/06 05:01:14 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-c865469a-2edf-4490-9d87-2756578173e4\n",
            "25/08/06 05:01:14 INFO MemoryStore: MemoryStore started with capacity 1767.6 MiB\n",
            "25/08/06 05:01:14 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "25/08/06 05:01:14 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "25/08/06 05:01:14 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "25/08/06 05:01:14 INFO Executor: Starting executor ID driver on host 294f2503e718\n",
            "25/08/06 05:01:14 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "25/08/06 05:01:14 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34675.\n",
            "25/08/06 05:01:14 INFO NettyBlockTransferService: Server created on 294f2503e718:34675\n",
            "25/08/06 05:01:14 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "25/08/06 05:01:14 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 294f2503e718, 34675, None)\n",
            "25/08/06 05:01:14 INFO BlockManagerMasterEndpoint: Registering block manager 294f2503e718:34675 with 1767.6 MiB RAM, BlockManagerId(driver, 294f2503e718, 34675, None)\n",
            "25/08/06 05:01:14 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 294f2503e718, 34675, None)\n",
            "25/08/06 05:01:14 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 294f2503e718, 34675, None)\n",
            "25/08/06 05:01:17 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "25/08/06 05:01:17 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "25/08/06 05:01:20 INFO CodeGenerator: Code generated in 680.800454 ms\n",
            "25/08/06 05:01:23 INFO CodeGenerator: Code generated in 23.184908 ms\n",
            "25/08/06 05:01:23 INFO CodeGenerator: Code generated in 20.357178 ms\n",
            "+------+\n",
            "| value|\n",
            "+------+\n",
            "|  Java|\n",
            "|Python|\n",
            "| Scala|\n",
            "+------+\n",
            "\n",
            "25/08/06 05:01:23 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "25/08/06 05:01:23 INFO SparkUI: Stopped Spark web UI at http://294f2503e718:4040\n",
            "25/08/06 05:01:23 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "25/08/06 05:01:23 INFO MemoryStore: MemoryStore cleared\n",
            "25/08/06 05:01:23 INFO BlockManager: BlockManager stopped\n",
            "25/08/06 05:01:23 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "25/08/06 05:01:23 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "25/08/06 05:01:23 INFO SparkContext: Successfully stopped SparkContext\n",
            "25/08/06 05:01:23 INFO ShutdownHookManager: Shutdown hook called\n",
            "25/08/06 05:01:23 INFO ShutdownHookManager: Deleting directory /tmp/spark-6f02a1d2-9275-41eb-8b75-309a01069d8c\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set varible\n",
        "\n",
        "!SPARK_HOME=/content/spark-3.3.2-bin-hadoop3\n",
        "!JARS=$(echo $SPARK_HOME/jars/*.jar | tr ' ' ':')\n"
      ],
      "metadata": {
        "id": "M4ClcALbla3J"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!echo $SPARK_HOME\n",
        "!echo $JARS"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-PmEhD51yxA",
        "outputId": "9699994b-978c-42ba-bd3f-8ccd6da3eee9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/spark-3.4.1-bin-hadoop3\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!java -cp \"$(echo $SPARK_HOME/jars/*.jar | tr ' ' ':')\" CreateDataFrame.java"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9urYc1e1_CI",
        "outputId": "5bc34f3a-3994-46c4-9b5e-aa313100498a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "25/08/06 05:01:44 INFO SparkContext: Running Spark version 3.4.1\n",
            "25/08/06 05:01:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "25/08/06 05:01:45 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 05:01:45 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "25/08/06 05:01:45 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 05:01:45 INFO SparkContext: Submitted application: CreateDataFrameExample\n",
            "25/08/06 05:01:45 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "25/08/06 05:01:45 INFO ResourceProfile: Limiting resource is cpu\n",
            "25/08/06 05:01:45 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "25/08/06 05:01:46 INFO SecurityManager: Changing view acls to: root\n",
            "25/08/06 05:01:46 INFO SecurityManager: Changing modify acls to: root\n",
            "25/08/06 05:01:46 INFO SecurityManager: Changing view acls groups to: \n",
            "25/08/06 05:01:46 INFO SecurityManager: Changing modify acls groups to: \n",
            "25/08/06 05:01:46 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "25/08/06 05:01:46 INFO Utils: Successfully started service 'sparkDriver' on port 38219.\n",
            "25/08/06 05:01:47 INFO SparkEnv: Registering MapOutputTracker\n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/content/spark-3.4.1-bin-hadoop3/jars/spark-unsafe_2.12-3.4.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "25/08/06 05:01:47 INFO SparkEnv: Registering BlockManagerMaster\n",
            "25/08/06 05:01:47 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "25/08/06 05:01:47 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "25/08/06 05:01:47 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "25/08/06 05:01:47 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-5b7794a1-f385-4010-ad67-db186f5608d9\n",
            "25/08/06 05:01:47 INFO MemoryStore: MemoryStore started with capacity 1767.6 MiB\n",
            "25/08/06 05:01:47 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "25/08/06 05:01:48 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "25/08/06 05:01:48 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "25/08/06 05:01:48 INFO Executor: Starting executor ID driver on host 294f2503e718\n",
            "25/08/06 05:01:48 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "25/08/06 05:01:48 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41297.\n",
            "25/08/06 05:01:48 INFO NettyBlockTransferService: Server created on 294f2503e718:41297\n",
            "25/08/06 05:01:48 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "25/08/06 05:01:48 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 294f2503e718, 41297, None)\n",
            "25/08/06 05:01:48 INFO BlockManagerMasterEndpoint: Registering block manager 294f2503e718:41297 with 1767.6 MiB RAM, BlockManagerId(driver, 294f2503e718, 41297, None)\n",
            "25/08/06 05:01:48 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 294f2503e718, 41297, None)\n",
            "25/08/06 05:01:48 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 294f2503e718, 41297, None)\n",
            "25/08/06 05:01:51 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "25/08/06 05:01:51 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "25/08/06 05:01:53 INFO CodeGenerator: Code generated in 476.985819 ms\n",
            "25/08/06 05:01:55 INFO CodeGenerator: Code generated in 33.643905 ms\n",
            "25/08/06 05:01:55 INFO CodeGenerator: Code generated in 22.079677 ms\n",
            "+------+\n",
            "| value|\n",
            "+------+\n",
            "|  Java|\n",
            "|Python|\n",
            "| Scala|\n",
            "+------+\n",
            "\n",
            "25/08/06 05:01:55 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "25/08/06 05:01:55 INFO SparkUI: Stopped Spark web UI at http://294f2503e718:4040\n",
            "25/08/06 05:01:55 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "25/08/06 05:01:55 INFO MemoryStore: MemoryStore cleared\n",
            "25/08/06 05:01:55 INFO BlockManager: BlockManager stopped\n",
            "25/08/06 05:01:55 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "25/08/06 05:01:55 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "25/08/06 05:01:55 INFO SparkContext: Successfully stopped SparkContext\n",
            "25/08/06 05:01:55 INFO ShutdownHookManager: Shutdown hook called\n",
            "25/08/06 05:01:55 INFO ShutdownHookManager: Deleting directory /tmp/spark-afcce83e-636e-448d-8387-5a0906eb71a7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!java -cp \".:$(echo $SPARK_HOME/jars/*.jar | tr ' ' ':')\" CreateDataFrame"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y69uEWm92J3S",
        "outputId": "8f994ff6-ca8a-411b-8396-69e5c2ba5944"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "25/08/06 04:31:56 INFO SparkContext: Running Spark version 3.4.1\n",
            "25/08/06 04:31:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "25/08/06 04:31:57 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 04:31:57 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "25/08/06 04:31:57 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 04:31:57 INFO SparkContext: Submitted application: CreateDataFrameExample\n",
            "25/08/06 04:31:57 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "25/08/06 04:31:57 INFO ResourceProfile: Limiting resource is cpu\n",
            "25/08/06 04:31:57 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "25/08/06 04:31:57 INFO SecurityManager: Changing view acls to: root\n",
            "25/08/06 04:31:57 INFO SecurityManager: Changing modify acls to: root\n",
            "25/08/06 04:31:57 INFO SecurityManager: Changing view acls groups to: \n",
            "25/08/06 04:31:57 INFO SecurityManager: Changing modify acls groups to: \n",
            "25/08/06 04:31:57 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "25/08/06 04:31:57 INFO Utils: Successfully started service 'sparkDriver' on port 44803.\n",
            "25/08/06 04:31:57 INFO SparkEnv: Registering MapOutputTracker\n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/content/spark-3.4.1-bin-hadoop3/jars/spark-unsafe_2.12-3.4.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "25/08/06 04:31:57 INFO SparkEnv: Registering BlockManagerMaster\n",
            "25/08/06 04:31:58 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "25/08/06 04:31:58 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "25/08/06 04:31:58 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "25/08/06 04:31:58 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-438e347e-679a-44cc-885a-6b9f32e390fc\n",
            "25/08/06 04:31:58 INFO MemoryStore: MemoryStore started with capacity 1767.6 MiB\n",
            "25/08/06 04:31:58 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "25/08/06 04:31:58 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "25/08/06 04:31:58 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "25/08/06 04:31:58 INFO Executor: Starting executor ID driver on host 921d066169b7\n",
            "25/08/06 04:31:59 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "25/08/06 04:31:59 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34639.\n",
            "25/08/06 04:31:59 INFO NettyBlockTransferService: Server created on 921d066169b7:34639\n",
            "25/08/06 04:31:59 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "25/08/06 04:31:59 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 921d066169b7, 34639, None)\n",
            "25/08/06 04:31:59 INFO BlockManagerMasterEndpoint: Registering block manager 921d066169b7:34639 with 1767.6 MiB RAM, BlockManagerId(driver, 921d066169b7, 34639, None)\n",
            "25/08/06 04:31:59 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 921d066169b7, 34639, None)\n",
            "25/08/06 04:31:59 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 921d066169b7, 34639, None)\n",
            "25/08/06 04:32:02 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "25/08/06 04:32:02 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "25/08/06 04:32:06 INFO CodeGenerator: Code generated in 654.355278 ms\n",
            "25/08/06 04:32:08 INFO CodeGenerator: Code generated in 36.013936 ms\n",
            "25/08/06 04:32:08 INFO CodeGenerator: Code generated in 25.784043 ms\n",
            "+------+\n",
            "| value|\n",
            "+------+\n",
            "|  Java|\n",
            "|Python|\n",
            "| Scala|\n",
            "+------+\n",
            "\n",
            "25/08/06 04:32:08 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "25/08/06 04:32:08 INFO SparkUI: Stopped Spark web UI at http://921d066169b7:4040\n",
            "25/08/06 04:32:08 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "25/08/06 04:32:08 INFO MemoryStore: MemoryStore cleared\n",
            "25/08/06 04:32:08 INFO BlockManager: BlockManager stopped\n",
            "25/08/06 04:32:08 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "25/08/06 04:32:08 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "25/08/06 04:32:08 INFO SparkContext: Successfully stopped SparkContext\n",
            "25/08/06 04:32:08 INFO ShutdownHookManager: Shutdown hook called\n",
            "25/08/06 04:32:08 INFO ShutdownHookManager: Deleting directory /tmp/spark-7b934dbf-5cd3-49e0-8cf9-d98a3ae8bc0c\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d0QvYqpTo2s1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e1a386a"
      },
      "source": [
        "# Task\n",
        "Perform data ingestion, setup, and product/order analysis using Apache Spark with Java based on the provided dataset tables and instructions. The analysis should include finding the top 10 products by quantity sold, calculating product-wise revenue, and calculating the average order value. The data should be read from CSV files, converted to Parquet, and stored in \"/data/parquet/\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf3a4c9a"
      },
      "source": [
        "## Environment setup\n",
        "\n",
        "### Subtask:\n",
        "Install Java and set up Apache Spark as described. Link the project with a GitHub repository.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01438b51"
      },
      "source": [
        "## Data ingestion & setup\n",
        "\n",
        "### Subtask:\n",
        "Read the CSV files for all the dataset tables into Spark DataFrames, define schemas, convert them to Parquet format, and save them to the specified location.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24d61dfd"
      },
      "source": [
        "**Reasoning**:\n",
        "Create a SparkSession object to start the Spark application.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7703e7e"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"CSV to Parquet Conversion\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9ba00c7"
      },
      "source": [
        "**Reasoning**:\n",
        "Define schemas for each table and read the CSV files into DataFrames using the defined schemas. Then convert and save the DataFrames to Parquet format.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0150d21d",
        "outputId": "94a30d61-c10d-421c-ce9f-b7ca61916497"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType, DateType\n",
        "import os\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"ClassicModels Ingestion\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Define manual schemas for each table\n",
        "schemas = {\n",
        "    \"customers\": StructType([\n",
        "        StructField(\"customerNumber\", IntegerType()),\n",
        "        StructField(\"customerName\", StringType()),\n",
        "        StructField(\"contactLastName\", StringType()),\n",
        "        StructField(\"contactFirstName\", StringType()),\n",
        "        StructField(\"phone\", StringType()),\n",
        "        StructField(\"addressLine1\", StringType()),\n",
        "        StructField(\"addressLine2\", StringType()),\n",
        "        StructField(\"city\", StringType()),\n",
        "        StructField(\"state\", StringType()),\n",
        "        StructField(\"postalCode\", StringType()),\n",
        "        StructField(\"country\", StringType()),\n",
        "        StructField(\"salesRepEmployeeNumber\", IntegerType()),\n",
        "        StructField(\"creditLimit\", DoubleType())\n",
        "    ]),\n",
        "    \"employees\": StructType([\n",
        "        StructField(\"employeeNumber\", IntegerType()),\n",
        "        StructField(\"lastName\", StringType()),\n",
        "        StructField(\"firstName\", StringType()),\n",
        "        StructField(\"extension\", StringType()),\n",
        "        StructField(\"email\", StringType()),\n",
        "        StructField(\"officeCode\", StringType()),\n",
        "        StructField(\"reportsTo\", IntegerType()),\n",
        "        StructField(\"jobTitle\", StringType())\n",
        "    ]),\n",
        "    \"offices\": StructType([\n",
        "        StructField(\"officeCode\", StringType()),\n",
        "        StructField(\"city\", StringType()),\n",
        "        StructField(\"phone\", StringType()),\n",
        "        StructField(\"addressLine1\", StringType()),\n",
        "        StructField(\"addressLine2\", StringType()),\n",
        "        StructField(\"state\", StringType()),\n",
        "        StructField(\"country\", StringType()),\n",
        "        StructField(\"postalCode\", StringType()),\n",
        "        StructField(\"territory\", StringType())\n",
        "    ]),\n",
        "    \"orderdetails\": StructType([\n",
        "        StructField(\"orderNumber\", IntegerType()),\n",
        "        StructField(\"productCode\", StringType()),\n",
        "        StructField(\"quantityOrdered\", IntegerType()),\n",
        "        StructField(\"priceEach\", DoubleType()),\n",
        "        StructField(\"orderLineNumber\", IntegerType())\n",
        "    ]),\n",
        "    \"orders\": StructType([\n",
        "        StructField(\"orderNumber\", IntegerType()),\n",
        "        StructField(\"orderDate\", DateType()),\n",
        "        StructField(\"requiredDate\", DateType()),\n",
        "        StructField(\"shippedDate\", DateType()),\n",
        "        StructField(\"status\", StringType()),\n",
        "        StructField(\"comments\", StringType()),\n",
        "        StructField(\"customerNumber\", IntegerType())\n",
        "    ]),\n",
        "    \"payments\": StructType([\n",
        "        StructField(\"customerNumber\", IntegerType()),\n",
        "        StructField(\"checkNumber\", StringType()),\n",
        "        StructField(\"paymentDate\", DateType()),\n",
        "        StructField(\"amount\", DoubleType())\n",
        "    ]),\n",
        "    \"productlines\": StructType([\n",
        "        StructField(\"productLine\", StringType()),\n",
        "        StructField(\"textDescription\", StringType()),\n",
        "        StructField(\"htmlDescription\", StringType()),\n",
        "        StructField(\"image\", StringType())\n",
        "    ]),\n",
        "    \"products\": StructType([\n",
        "        StructField(\"productCode\", StringType()),\n",
        "        StructField(\"productName\", StringType()),\n",
        "        StructField(\"productLine\", StringType()),\n",
        "        StructField(\"productScale\", StringType()),\n",
        "        StructField(\"productVendor\", StringType()),\n",
        "        StructField(\"productDescription\", StringType()),\n",
        "        StructField(\"quantityInStock\", IntegerType()),\n",
        "        StructField(\"buyPrice\", DoubleType()),\n",
        "        StructField(\"MSRP\", DoubleType())\n",
        "    ])\n",
        "}\n",
        "\n",
        "# Input: CSV folder in Drive\n",
        "base_input_path = \"/content/drive/MyDrive/classicmodels\"\n",
        "\n",
        "# Output: Parquet folder in Drive (persistent)\n",
        "base_output_path = \"/content/drive/MyDrive/data/parquet/\"\n",
        "\n",
        "# Debug: Check input directory\n",
        "print(f\"Checking input path: {base_input_path}\")\n",
        "if os.path.exists(base_input_path):\n",
        "    print(f\"✅ Input path exists.\\nContents: {os.listdir(base_input_path)}\\n\")\n",
        "else:\n",
        "    print(\"❌ ERROR: Input path does NOT exist. Check your Drive folder name.\")\n",
        "    raise FileNotFoundError(base_input_path)\n",
        "\n",
        "# Read each CSV, convert to Parquet\n",
        "for table_name, schema in schemas.items():\n",
        "    csv_path = os.path.join(base_input_path, f\"{table_name}.csv\")\n",
        "    parquet_path = os.path.join(base_output_path, table_name)\n",
        "\n",
        "    print(f\"🔄 Processing: {table_name}\")\n",
        "\n",
        "    # Check if CSV file exists\n",
        "    if not os.path.exists(csv_path):\n",
        "        print(f\"⚠️  Skipped: File not found → {csv_path}\")\n",
        "        continue\n",
        "\n",
        "    # Read with schema\n",
        "    df = spark.read.csv(csv_path, header=True, schema=schema)\n",
        "\n",
        "    # Save to Parquet\n",
        "    df.write.mode(\"overwrite\").parquet(parquet_path)\n",
        "    print(f\"✅ Saved {table_name} to {parquet_path}\\n\")\n",
        "\n",
        "print(\"🎉 All available CSVs processed and saved as Parquet.\")\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking input path: /content/drive/MyDrive/classicmodels\n",
            "✅ Input path exists.\n",
            "Contents: ['products.csv', 'offices.csv', 'productlines.csv', 'employees.csv', 'payments.csv', 'customers.csv', 'orders.csv', 'orderdetails.csv', 'customers', 'employees', 'offices', 'orderdetails', 'orders', 'payments', 'productlines', 'products']\n",
            "\n",
            "🔄 Processing: customers\n",
            "✅ Saved customers to /content/drive/MyDrive/data/parquet/customers\n",
            "\n",
            "🔄 Processing: employees\n",
            "✅ Saved employees to /content/drive/MyDrive/data/parquet/employees\n",
            "\n",
            "🔄 Processing: offices\n",
            "✅ Saved offices to /content/drive/MyDrive/data/parquet/offices\n",
            "\n",
            "🔄 Processing: orderdetails\n",
            "✅ Saved orderdetails to /content/drive/MyDrive/data/parquet/orderdetails\n",
            "\n",
            "🔄 Processing: orders\n",
            "✅ Saved orders to /content/drive/MyDrive/data/parquet/orders\n",
            "\n",
            "🔄 Processing: payments\n",
            "✅ Saved payments to /content/drive/MyDrive/data/parquet/payments\n",
            "\n",
            "🔄 Processing: productlines\n",
            "✅ Saved productlines to /content/drive/MyDrive/data/parquet/productlines\n",
            "\n",
            "🔄 Processing: products\n",
            "✅ Saved products to /content/drive/MyDrive/data/parquet/products\n",
            "\n",
            "🎉 All available CSVs processed and saved as Parquet.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cb621e5a",
        "outputId": "4462b0b9-cd2d-4add-c2a4-04295292289f"
      },
      "source": [
        "# Read CSVs, convert to Parquet, and save\n",
        "import os\n",
        "\n",
        "base_input_path = \"/content/drive/MyDrive/classicmodels\"\n",
        "base_output_path = \"/content/drive/MyDrive/data/parquet/\"\n",
        "\n",
        "# Verify the input directory exists and list its contents for debugging\n",
        "print(f\"Checking input path: {base_input_path}\")\n",
        "if os.path.exists(base_input_path):\n",
        "    print(f\"✅ Input path exists. Contents: {os.listdir(base_input_path)}\\n\")\n",
        "else:\n",
        "    print(f\"❌ Input path does NOT exist.\")\n",
        "    raise FileNotFoundError(base_input_path)\n",
        "\n",
        "# Iterate through all tables and convert to Parquet\n",
        "for table_name, schema in schemas.items():\n",
        "    csv_path = os.path.join(base_input_path, f\"{table_name}.csv\")\n",
        "    parquet_path = os.path.join(base_output_path, table_name)\n",
        "\n",
        "    print(f\"🔄 Processing {table_name}...\")\n",
        "\n",
        "    if not os.path.exists(csv_path):\n",
        "        print(f\"⚠️  Skipped: Input file not found: {csv_path}\")\n",
        "        continue\n",
        "\n",
        "    df = spark.read.csv(csv_path, header=True, schema=schema)\n",
        "\n",
        "    # Create output folder (Drive) if needed\n",
        "    os.makedirs(parquet_path, exist_ok=True)\n",
        "\n",
        "    df.write.parquet(parquet_path, mode=\"overwrite\")\n",
        "    print(f\"✅ Saved {table_name} to {parquet_path}\\n\")\n",
        "\n",
        "print(\"🎉 All CSVs processed and saved as Parquet in Google Drive.\")\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking input path: /content/drive/MyDrive/classicmodels\n",
            "✅ Input path exists. Contents: ['products.csv', 'offices.csv', 'productlines.csv', 'employees.csv', 'payments.csv', 'customers.csv', 'orders.csv', 'orderdetails.csv', 'customers', 'employees', 'offices', 'orderdetails', 'orders', 'payments', 'productlines', 'products']\n",
            "\n",
            "🔄 Processing customers...\n",
            "✅ Saved customers to /content/drive/MyDrive/data/parquet/customers\n",
            "\n",
            "🔄 Processing employees...\n",
            "✅ Saved employees to /content/drive/MyDrive/data/parquet/employees\n",
            "\n",
            "🔄 Processing offices...\n",
            "✅ Saved offices to /content/drive/MyDrive/data/parquet/offices\n",
            "\n",
            "🔄 Processing orderdetails...\n",
            "✅ Saved orderdetails to /content/drive/MyDrive/data/parquet/orderdetails\n",
            "\n",
            "🔄 Processing orders...\n",
            "✅ Saved orders to /content/drive/MyDrive/data/parquet/orders\n",
            "\n",
            "🔄 Processing payments...\n",
            "✅ Saved payments to /content/drive/MyDrive/data/parquet/payments\n",
            "\n",
            "🔄 Processing productlines...\n",
            "✅ Saved productlines to /content/drive/MyDrive/data/parquet/productlines\n",
            "\n",
            "🔄 Processing products...\n",
            "✅ Saved products to /content/drive/MyDrive/data/parquet/products\n",
            "\n",
            "🎉 All CSVs processed and saved as Parquet in Google Drive.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1750c99c",
        "outputId": "7b11082e-a19a-4c73-d559-bcccf4536c7c"
      },
      "source": [
        "# Read CSVs, convert to Parquet, and save to Google Drive\n",
        "\n",
        "import os\n",
        "\n",
        "# ✅ FIXED: Base paths\n",
        "base_input_path = \"/content/drive/MyDrive/classicmodels\"\n",
        "base_output_path = \"/content/drive/MyDrive/data/parquet/\"\n",
        "\n",
        "# Verify the input directory exists and list its contents\n",
        "print(f\"Checking input path: {base_input_path}\")\n",
        "if os.path.exists(base_input_path):\n",
        "    print(f\"✅ Input path exists. Contents: {os.listdir(base_input_path)}\\n\")\n",
        "else:\n",
        "    print(f\"❌ Input path does NOT exist.\")\n",
        "    raise FileNotFoundError(base_input_path)\n",
        "\n",
        "# Iterate through schemas and convert CSV → Parquet\n",
        "for table_name, schema in schemas.items():\n",
        "    csv_path = os.path.join(base_input_path, f\"{table_name}.csv\")\n",
        "    parquet_path = os.path.join(base_output_path, table_name)\n",
        "\n",
        "    print(f\"🔄 Processing {table_name}...\")\n",
        "\n",
        "    if not os.path.exists(csv_path):\n",
        "        print(f\"⚠️  Skipped: Input file not found: {csv_path}\")\n",
        "        continue\n",
        "\n",
        "    df = spark.read.csv(csv_path, header=True, schema=schema)\n",
        "\n",
        "    os.makedirs(parquet_path, exist_ok=True)\n",
        "\n",
        "    df.write.mode(\"overwrite\").parquet(parquet_path)\n",
        "    print(f\"✅ Saved {table_name} to {parquet_path}\\n\")\n",
        "\n",
        "print(\"🎉 All CSV files converted to Parquet and saved in Google Drive.\")\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking input path: /content/drive/MyDrive/classicmodels\n",
            "✅ Input path exists. Contents: ['products.csv', 'offices.csv', 'productlines.csv', 'employees.csv', 'payments.csv', 'customers.csv', 'orders.csv', 'orderdetails.csv', 'customers', 'employees', 'offices', 'orderdetails', 'orders', 'payments', 'productlines', 'products']\n",
            "\n",
            "🔄 Processing customers...\n",
            "✅ Saved customers to /content/drive/MyDrive/data/parquet/customers\n",
            "\n",
            "🔄 Processing employees...\n",
            "✅ Saved employees to /content/drive/MyDrive/data/parquet/employees\n",
            "\n",
            "🔄 Processing offices...\n",
            "✅ Saved offices to /content/drive/MyDrive/data/parquet/offices\n",
            "\n",
            "🔄 Processing orderdetails...\n",
            "✅ Saved orderdetails to /content/drive/MyDrive/data/parquet/orderdetails\n",
            "\n",
            "🔄 Processing orders...\n",
            "✅ Saved orders to /content/drive/MyDrive/data/parquet/orders\n",
            "\n",
            "🔄 Processing payments...\n",
            "✅ Saved payments to /content/drive/MyDrive/data/parquet/payments\n",
            "\n",
            "🔄 Processing productlines...\n",
            "✅ Saved productlines to /content/drive/MyDrive/data/parquet/productlines\n",
            "\n",
            "🔄 Processing products...\n",
            "✅ Saved products to /content/drive/MyDrive/data/parquet/products\n",
            "\n",
            "🎉 All CSV files converted to Parquet and saved in Google Drive.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "009fc819"
      },
      "source": [
        "**Reasoning**:\n",
        "Load the 'orderdetails' and 'products' Parquet files into Spark DataFrames.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e00c1f62",
        "outputId": "6d704a6c-59ac-40d4-b9c0-130f71bb2384"
      },
      "source": [
        "orderdetails_df = spark.read.parquet(\"/content/drive/MyDrive/data/parquet/orderdetails\")\n",
        "products_df = spark.read.parquet(\"/content/drive/MyDrive/data/parquet/products\")\n",
        "\n",
        "orderdetails_df.show(5)\n",
        "products_df.show(5)\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----------+---------------+---------+---------------+\n",
            "|orderNumber|productCode|quantityOrdered|priceEach|orderLineNumber|\n",
            "+-----------+-----------+---------------+---------+---------------+\n",
            "|       null|productCode|           null|     null|           null|\n",
            "|      10100|   S18_1749|             30|    136.0|              3|\n",
            "|      10100|   S18_2248|             50|    55.09|              2|\n",
            "|      10100|   S18_4409|             22|    75.46|              4|\n",
            "|      10100|   S24_3969|             49|    35.29|              1|\n",
            "+-----------+-----------+---------------+---------+---------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+-----------+--------------------+-----------+------------+---------------+--------------------+---------------+--------+----+\n",
            "|productCode|         productName|productLine|productScale|  productVendor|  productDescription|quantityInStock|buyPrice|MSRP|\n",
            "+-----------+--------------------+-----------+------------+---------------+--------------------+---------------+--------+----+\n",
            "|productCode|         productName|productLine|productScale|  productVendor|  productDescription|           null|    null|null|\n",
            "|   S10_1678|1969 Harley David...|Motorcycles|        1:10|Min Lin Diecast|This replica feat...|           7933|   48.81|95.7|\n",
            "+-----------+--------------------+-----------+------------+---------------+--------------------+---------------+--------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50e6a65b",
        "outputId": "b36a6003-ad7a-4d0d-ab12-63a533eed34a"
      },
      "source": [
        "orderdetails_df = spark.read.parquet(\"/content/drive/MyDrive/data/parquet/orderdetails\")\n",
        "products_df = spark.read.parquet(\"/content/drive/MyDrive/data/parquet/products\")\n",
        "orderdetails_df.show(5)\n",
        "products_df.show(5)\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----------+---------------+---------+---------------+\n",
            "|orderNumber|productCode|quantityOrdered|priceEach|orderLineNumber|\n",
            "+-----------+-----------+---------------+---------+---------------+\n",
            "|       null|productCode|           null|     null|           null|\n",
            "|      10100|   S18_1749|             30|    136.0|              3|\n",
            "|      10100|   S18_2248|             50|    55.09|              2|\n",
            "|      10100|   S18_4409|             22|    75.46|              4|\n",
            "|      10100|   S24_3969|             49|    35.29|              1|\n",
            "+-----------+-----------+---------------+---------+---------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+-----------+--------------------+-----------+------------+---------------+--------------------+---------------+--------+----+\n",
            "|productCode|         productName|productLine|productScale|  productVendor|  productDescription|quantityInStock|buyPrice|MSRP|\n",
            "+-----------+--------------------+-----------+------------+---------------+--------------------+---------------+--------+----+\n",
            "|productCode|         productName|productLine|productScale|  productVendor|  productDescription|           null|    null|null|\n",
            "|   S10_1678|1969 Harley David...|Motorcycles|        1:10|Min Lin Diecast|This replica feat...|           7933|   48.81|95.7|\n",
            "+-----------+--------------------+-----------+------------+---------------+--------------------+---------------+--------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsYdeTHTr6-_",
        "outputId": "1e8f1056-7149-42c7-8a59-d7a3dca2defe"
      },
      "source": [
        "!ls /content/drive/MyDrive/data/parquet/\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "customers  offices\t orders    productlines\n",
            "employees  orderdetails  payments  products\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9c967594",
        "outputId": "bc41b895-9ef7-472b-ea74-1c80b19ab37b"
      },
      "source": [
        "import os\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType, DateType\n",
        "\n",
        "# ✅ CORRECT paths (Google Drive)\n",
        "base_input_path = \"/content/drive/MyDrive/classicmodels\"\n",
        "base_output_path = \"/content/drive/MyDrive/data/parquet\"\n",
        "\n",
        "# ✅ Start Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"CSV to Parquet Conversion\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# ✅ Define schema for each table\n",
        "schemas = {\n",
        "    \"customers\": StructType([\n",
        "        StructField(\"customerNumber\", IntegerType(), True),\n",
        "        StructField(\"customerName\", StringType(), True),\n",
        "        StructField(\"contactLastName\", StringType(), True),\n",
        "        StructField(\"contactFirstName\", StringType(), True),\n",
        "        StructField(\"phone\", StringType(), True),\n",
        "        StructField(\"addressLine1\", StringType(), True),\n",
        "        StructField(\"addressLine2\", StringType(), True),\n",
        "        StructField(\"city\", StringType(), True),\n",
        "        StructField(\"state\", StringType(), True),\n",
        "        StructField(\"postalCode\", StringType(), True),\n",
        "        StructField(\"country\", StringType(), True),\n",
        "        StructField(\"salesRepEmployeeNumber\", IntegerType(), True),\n",
        "        StructField(\"creditLimit\", DoubleType(), True)\n",
        "    ]),\n",
        "    \"employees\": StructType([\n",
        "        StructField(\"employeeNumber\", IntegerType(), True),\n",
        "        StructField(\"lastName\", StringType(), True),\n",
        "        StructField(\"firstName\", StringType(), True),\n",
        "        StructField(\"extension\", StringType(), True),\n",
        "        StructField(\"email\", StringType(), True),\n",
        "        StructField(\"officeCode\", StringType(), True),\n",
        "        StructField(\"reportsTo\", IntegerType(), True),\n",
        "        StructField(\"jobTitle\", StringType(), True)\n",
        "    ]),\n",
        "    \"offices\": StructType([\n",
        "        StructField(\"officeCode\", StringType(), True),\n",
        "        StructField(\"city\", StringType(), True),\n",
        "        StructField(\"phone\", StringType(), True),\n",
        "        StructField(\"addressLine1\", StringType(), True),\n",
        "        StructField(\"addressLine2\", StringType(), True),\n",
        "        StructField(\"state\", StringType(), True),\n",
        "        StructField(\"country\", StringType(), True),\n",
        "        StructField(\"postalCode\", StringType(), True),\n",
        "        StructField(\"territory\", StringType(), True)\n",
        "    ]),\n",
        "    \"orderdetails\": StructType([\n",
        "        StructField(\"orderNumber\", IntegerType(), True),\n",
        "        StructField(\"productCode\", StringType(), True),\n",
        "        StructField(\"quantityOrdered\", IntegerType(), True),\n",
        "        StructField(\"priceEach\", DoubleType(), True),\n",
        "        StructField(\"orderLineNumber\", IntegerType(), True)\n",
        "    ]),\n",
        "    \"orders\": StructType([\n",
        "        StructField(\"orderNumber\", IntegerType(), True),\n",
        "        StructField(\"orderDate\", DateType(), True),\n",
        "        StructField(\"requiredDate\", DateType(), True),\n",
        "        StructField(\"shippedDate\", DateType(), True),\n",
        "        StructField(\"status\", StringType(), True),\n",
        "        StructField(\"comments\", StringType(), True),\n",
        "        StructField(\"customerNumber\", IntegerType(), True)\n",
        "    ]),\n",
        "    \"payments\": StructType([\n",
        "        StructField(\"customerNumber\", IntegerType(), True),\n",
        "        StructField(\"checkNumber\", StringType(), True),\n",
        "        StructField(\"paymentDate\", DateType(), True),\n",
        "        StructField(\"amount\", DoubleType(), True)\n",
        "    ]),\n",
        "    \"productlines\": StructType([\n",
        "        StructField(\"productLine\", StringType(), True),\n",
        "        StructField(\"textDescription\", StringType(), True),\n",
        "        StructField(\"htmlDescription\", StringType(), True),\n",
        "        StructField(\"image\", StringType(), True)\n",
        "    ]),\n",
        "    \"products\": StructType([\n",
        "        StructField(\"productCode\", StringType(), True),\n",
        "        StructField(\"productName\", StringType(), True),\n",
        "        StructField(\"productLine\", StringType(), True),\n",
        "        StructField(\"productScale\", StringType(), True),\n",
        "        StructField(\"productVendor\", StringType(), True),\n",
        "        StructField(\"productDescription\", StringType(), True),\n",
        "        StructField(\"quantityInStock\", IntegerType(), True),\n",
        "        StructField(\"buyPrice\", DoubleType(), True),\n",
        "        StructField(\"MSRP\", DoubleType(), True)\n",
        "    ])\n",
        "}\n",
        "\n",
        "# ✅ Check input directory\n",
        "print(f\"Checking input path: {base_input_path}\")\n",
        "if os.path.exists(base_input_path):\n",
        "    print(f\"Input path exists. Contents: {os.listdir(base_input_path)}\")\n",
        "else:\n",
        "    print(\"Input path does NOT exist.\")\n",
        "\n",
        "# ✅ Loop over each table\n",
        "for table_name, schema in schemas.items():\n",
        "    csv_path = os.path.join(base_input_path, f\"{table_name}.csv\")\n",
        "    parquet_path = os.path.join(base_output_path, table_name)\n",
        "\n",
        "    print(f\"\\nProcessing {table_name} from: {csv_path}\")\n",
        "\n",
        "    if not os.path.exists(csv_path):\n",
        "        print(f\"❌ File not found: {csv_path}\")\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        df = spark.read.csv(csv_path, header=True, schema=schema)\n",
        "\n",
        "        # Make sure parent directory exists\n",
        "        os.makedirs(parquet_path, exist_ok=True)\n",
        "\n",
        "        df.write.parquet(parquet_path, mode=\"overwrite\")\n",
        "        print(f\"✅ Saved to: {parquet_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error processing {table_name}: {e}\")\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking input path: /content/drive/MyDrive/classicmodels\n",
            "Input path exists. Contents: ['products.csv', 'offices.csv', 'productlines.csv', 'employees.csv', 'payments.csv', 'customers.csv', 'orders.csv', 'orderdetails.csv', 'customers', 'employees', 'offices', 'orderdetails', 'orders', 'payments', 'productlines', 'products']\n",
            "\n",
            "Processing customers from: /content/drive/MyDrive/classicmodels/customers.csv\n",
            "✅ Saved to: /content/drive/MyDrive/data/parquet/customers\n",
            "\n",
            "Processing employees from: /content/drive/MyDrive/classicmodels/employees.csv\n",
            "✅ Saved to: /content/drive/MyDrive/data/parquet/employees\n",
            "\n",
            "Processing offices from: /content/drive/MyDrive/classicmodels/offices.csv\n",
            "✅ Saved to: /content/drive/MyDrive/data/parquet/offices\n",
            "\n",
            "Processing orderdetails from: /content/drive/MyDrive/classicmodels/orderdetails.csv\n",
            "✅ Saved to: /content/drive/MyDrive/data/parquet/orderdetails\n",
            "\n",
            "Processing orders from: /content/drive/MyDrive/classicmodels/orders.csv\n",
            "✅ Saved to: /content/drive/MyDrive/data/parquet/orders\n",
            "\n",
            "Processing payments from: /content/drive/MyDrive/classicmodels/payments.csv\n",
            "✅ Saved to: /content/drive/MyDrive/data/parquet/payments\n",
            "\n",
            "Processing productlines from: /content/drive/MyDrive/classicmodels/productlines.csv\n",
            "✅ Saved to: /content/drive/MyDrive/data/parquet/productlines\n",
            "\n",
            "Processing products from: /content/drive/MyDrive/classicmodels/products.csv\n",
            "✅ Saved to: /content/drive/MyDrive/data/parquet/products\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top_10_products_df = joined_df.groupBy(\"productName\") \\\n",
        "                             .agg(sum(\"quantityOrdered\").alias(\"totalQuantitySold\")) \\\n",
        "                             .orderBy(\"totalQuantitySold\", ascending=False) \\\n",
        "                             .limit(10)\n"
      ],
      "metadata": {
        "id": "a_LBtoEp1EYO"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, round, sum as _sum\n",
        "import os\n",
        "\n",
        "# Define input and output paths\n",
        "input_path = \"/data/parquet\"\n",
        "output_path = \"/output/processed\"\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "# Read the required Parquet files\n",
        "orders_df = spark.read.parquet(f\"{input_path}/orders\")\n",
        "orderdetails_df = spark.read.parquet(f\"{input_path}/orderdetails\")\n",
        "products_df = spark.read.parquet(f\"{input_path}/products\")\n",
        "\n",
        "# Join orderdetails with orders on orderNumber\n",
        "order_with_details = orderdetails_df.join(orders_df, \"orderNumber\")\n",
        "\n",
        "# Join the above with products on productCode\n",
        "full_joined_df = order_with_details.join(products_df, \"productCode\")\n",
        "\n",
        "# Calculate revenue = quantityOrdered * priceEach\n",
        "revenue_df = full_joined_df.withColumn(\"revenue\", col(\"quantityOrdered\") * col(\"priceEach\"))\n",
        "\n",
        "# Group by productName and sum the revenue\n",
        "product_revenue_df = revenue_df.groupBy(\"productName\") \\\n",
        "    .agg(round(_sum(\"revenue\"), 2).alias(\"totalRevenue\")) \\\n",
        "    .orderBy(col(\"totalRevenue\").desc())\n",
        "\n",
        "# Show top 10 for visual check\n",
        "product_revenue_df.show(10, truncate=False)\n",
        "\n",
        "# Save output as Parquet\n",
        "product_revenue_df.write.mode(\"overwrite\").parquet(f\"{output_path}/product_revenue.parquet\")\n",
        "print(\"✅ Saved product-wise revenue to /output/processed/product_revenue.parquet\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGbF7GD41Eou",
        "outputId": "4120dd2a-559c-4a78-d841-d4396ca192e1"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------------------------+------------+\n",
            "|productName                          |totalRevenue|\n",
            "+-------------------------------------+------------+\n",
            "|1969 Harley Davidson Ultimate Chopper|16527.99    |\n",
            "+-------------------------------------+------------+\n",
            "\n",
            "✅ Saved product-wise revenue to /output/processed/product_revenue.parquet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, round, sum as _sum, count\n",
        "\n",
        "# Define input/output paths\n",
        "input_path = \"/content/drive/MyDrive/classicmodels\"\n",
        "output_path = \"/output/processed\"\n",
        "\n",
        "# Read required datasets\n",
        "customers_df = spark.read.parquet(f\"{input_path}/customers\")\n",
        "orders_df = spark.read.parquet(f\"{input_path}/orders\")\n",
        "orderdetails_df = spark.read.parquet(f\"{input_path}/orderdetails\")\n",
        "\n",
        "# Join orderdetails with orders on orderNumber\n",
        "order_data = orderdetails_df.join(orders_df, \"orderNumber\")\n",
        "\n",
        "# Calculate revenue for each line item\n",
        "order_data = order_data.withColumn(\"lineRevenue\", col(\"quantityOrdered\") * col(\"priceEach\"))\n",
        "\n",
        "# Calculate total revenue per order\n",
        "order_total_df = order_data.groupBy(\"orderNumber\", \"customerNumber\") \\\n",
        "    .agg(_sum(\"lineRevenue\").alias(\"orderTotal\"))\n",
        "\n",
        "# Join with customers to get names\n",
        "order_customer_df = order_total_df.join(customers_df, \"customerNumber\")\n",
        "\n",
        "# Group by customer and calculate average order value\n",
        "customer_avg_order_df = order_customer_df.groupBy(\"customerNumber\", \"customerName\") \\\n",
        "    .agg(round(_sum(\"orderTotal\") / count(\"orderNumber\"), 2).alias(\"avgOrderValue\")) \\\n",
        "    .orderBy(col(\"avgOrderValue\").desc())\n",
        "\n",
        "# Show top 10 only\n",
        "customer_avg_order_df.show(10, truncate=False)\n",
        "\n",
        "# Save full result to Parquet\n",
        "customer_avg_order_df.write.mode(\"overwrite\").parquet(f\"{output_path}/customer_avg_order.parquet\")\n",
        "print(\"✅ Saved full customer average order values to /output/processed/customer_avg_order.parquet\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQ7AMbin1Erh",
        "outputId": "4378e76b-ca4f-48bc-8c07-0d73fdf3fa40"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+----------------------------+-------------+\n",
            "|customerNumber|customerName                |avgOrderValue|\n",
            "+--------------+----------------------------+-------------+\n",
            "|151           |Muscle Machine Inc          |58841.35     |\n",
            "|145           |Danish Wholesale Imports    |53959.21     |\n",
            "|278           |Rovelli Gifts               |52151.81     |\n",
            "|385           |Cruz & Sons Co.             |51001.22     |\n",
            "|350           |Marseille Mini Autos        |50824.66     |\n",
            "|205           |Toys4GrownUps.com           |50342.74     |\n",
            "|321           |Corporate Gift Ideas Co.    |42779.56     |\n",
            "|148           |Dragon Souveniers, Ltd.     |41365.15     |\n",
            "|320           |Mini Creations Ltd.         |41016.75     |\n",
            "|124           |Mini Gifts Distributors Ltd.|40899.57     |\n",
            "+--------------+----------------------------+-------------+\n",
            "only showing top 10 rows\n",
            "\n",
            "✅ Saved full customer average order values to /output/processed/customer_avg_order.parquet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "customer_avg_order_df = order_customer_df.groupBy(\"customerNumber\", \"customerName\") \\\n",
        "    .agg(round(_sum(\"orderTotal\") / count(\"orderNumber\"), 2).alias(\"avgOrderValue\")) \\\n",
        "    .orderBy(col(\"avgOrderValue\").desc())\n"
      ],
      "metadata": {
        "id": "wtUjMt8K1Et1"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, sum as _sum\n",
        "\n",
        "# Paths\n",
        "base_path = \"/content/drive/MyDrive/classicmodels\"\n",
        "output_path = \"/output/processed\"\n",
        "\n",
        "# Step 1: Read required tables\n",
        "offices_df = spark.read.parquet(f\"{base_path}/offices\")\n",
        "employees_df = spark.read.parquet(f\"{base_path}/employees\")\n",
        "customers_df = spark.read.parquet(f\"{base_path}/customers\")\n",
        "orders_df = spark.read.parquet(f\"{base_path}/orders\")\n",
        "orderdetails_df = spark.read.parquet(f\"{base_path}/orderdetails\")\n",
        "\n",
        "# Step 2: Join offices -> employees\n",
        "emp_office_df = employees_df.join(offices_df, \"officeCode\")\n",
        "\n",
        "# Step 3: Join employees -> customers (via salesRepEmployeeNumber)\n",
        "emp_cust_df = customers_df.join(emp_office_df, customers_df[\"salesRepEmployeeNumber\"] == emp_office_df[\"employeeNumber\"])\n",
        "\n",
        "# Step 4: Join customers -> orders\n",
        "cust_orders_df = emp_cust_df.join(orders_df, \"customerNumber\")\n",
        "\n",
        "# Step 5: Join orders -> orderdetails\n",
        "full_sales_df = cust_orders_df.join(orderdetails_df, \"orderNumber\")\n",
        "\n",
        "# Step 6: Compute revenue\n",
        "full_sales_df = full_sales_df.withColumn(\"revenue\", col(\"quantityOrdered\") * col(\"priceEach\"))\n",
        "\n",
        "# Step 7: Group by region (country + city)\n",
        "regional_sales_df = full_sales_df.groupBy(\n",
        "    offices_df[\"country\"], offices_df[\"city\"]\n",
        ").agg(\n",
        "    _sum(\"revenue\").alias(\"totalSales\")\n",
        ").orderBy(\n",
        "    col(\"totalSales\").desc()\n",
        ")\n",
        "\n",
        "# Step 8: Show top regions\n",
        "regional_sales_df.show(10, truncate=False)\n",
        "\n",
        "# Step 9: Save full result to Parquet\n",
        "regional_sales_df.write.mode(\"overwrite\").parquet(f\"{output_path}/regional_sales_by_location.parquet\")\n",
        "print(\"✅ Regional sales by location saved to /output/processed/regional_sales_by_location.parquet\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "huMICNli1EwS",
        "outputId": "158a15df-8d74-411b-ebd0-270ffeabf208"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------------+------------------+\n",
            "|country  |city         |totalSales        |\n",
            "+---------+-------------+------------------+\n",
            "|USA      |San Francisco|380918.45999999996|\n",
            "|France   |Paris        |329517.0699999999 |\n",
            "|USA      |NYC          |179833.89         |\n",
            "|UK       |London       |162301.72         |\n",
            "|Japan    |Tokyo        |133731.51999999996|\n",
            "|Australia|Sydney       |122221.39         |\n",
            "|USA      |Boston       |89957.85          |\n",
            "+---------+-------------+------------------+\n",
            "\n",
            "✅ Regional sales by location saved to /output/processed/regional_sales_by_location.parquet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import sum as _sum\n",
        "\n",
        "# Step 1: Read customers and payments\n",
        "customers_df = spark.read.parquet(f\"{input_path}/customers\")\n",
        "payments_df = spark.read.parquet(f\"{input_path}/payments\")\n",
        "\n",
        "# Step 2: Join customers and payments on customerNumber\n",
        "customer_payments_df = payments_df.join(customers_df, \"customerNumber\")\n",
        "\n",
        "# Step 3: Group by country and sum the payment amounts\n",
        "revenue_by_country_df = customer_payments_df.groupBy(\"country\") \\\n",
        "    .agg(_sum(\"amount\").alias(\"totalRevenue\")) \\\n",
        "    .orderBy(col(\"totalRevenue\").desc())\n",
        "\n",
        "# Step 4: Show top countries\n",
        "revenue_by_country_df.show(10, truncate=False)\n",
        "\n",
        "# Step 5: Save result to Parquet\n",
        "revenue_by_country_df.write.mode(\"overwrite\").parquet(f\"{output_path}/revenue_by_country.parquet\")\n",
        "print(\"✅ Revenue by country saved to /output/processed/revenue_by_country.parquet\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IpfLjUWe1Eyy",
        "outputId": "a5cbf9bb-6f00-4edc-f76e-8df69a8a1c73"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+------------------+\n",
            "|country    |totalRevenue      |\n",
            "+-----------+------------------+\n",
            "|USA        |2998523.3299999996|\n",
            "|France     |792760.0900000003 |\n",
            "|New Zealand|392486.59         |\n",
            "|Australia  |372351.6          |\n",
            "|Italy      |325254.55000000005|\n",
            "|Finland    |295149.35         |\n",
            "|Singapore  |261671.59999999998|\n",
            "|Denmark    |197356.3          |\n",
            "|Germany    |196470.99         |\n",
            "|Japan      |167909.95         |\n",
            "+-----------+------------------+\n",
            "only showing top 10 rows\n",
            "\n",
            "✅ Revenue by country saved to /output/processed/revenue_by_country.parquet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import sum as _sum\n",
        "\n",
        "# Step 1: Read all required tables\n",
        "offices_df = spark.read.parquet(f\"{input_path}/offices\")\n",
        "employees_df = spark.read.parquet(f\"{input_path}/employees\")\n",
        "customers_df = spark.read.parquet(f\"{input_path}/customers\")\n",
        "payments_df = spark.read.parquet(f\"{input_path}/payments\")\n",
        "\n",
        "# Step 2: Join employees with offices\n",
        "emp_office_df = employees_df.join(offices_df, \"officeCode\")\n",
        "\n",
        "# Step 3: Join customers with employees (salesRepEmployeeNumber)\n",
        "cust_emp_office_df = customers_df.join(\n",
        "    emp_office_df,\n",
        "    customers_df[\"salesRepEmployeeNumber\"] == emp_office_df[\"employeeNumber\"]\n",
        ")\n",
        "\n",
        "# Step 4: Join payments with full hierarchy\n",
        "full_office_sales_df = payments_df.join(cust_emp_office_df, \"customerNumber\")\n",
        "\n",
        "# Step 5: Select only the office columns (rename to avoid ambiguity)\n",
        "office_sales_clean_df = full_office_sales_df.select(\n",
        "    \"officeCode\",\n",
        "    emp_office_df[\"city\"].alias(\"officeCity\"),\n",
        "    emp_office_df[\"country\"].alias(\"officeCountry\"),\n",
        "    \"amount\"\n",
        ")\n",
        "\n",
        "# Step 6: Group by office and calculate total sales\n",
        "office_sales_df = office_sales_clean_df.groupBy(\"officeCode\", \"officeCity\", \"officeCountry\") \\\n",
        "    .agg(_sum(\"amount\").alias(\"totalSales\")) \\\n",
        "    .orderBy(col(\"totalSales\").desc())\n",
        "\n",
        "# Step 7: Show results\n",
        "office_sales_df.show(truncate=False)\n",
        "\n",
        "# Step 8: Save to Parquet\n",
        "office_sales_df.write.mode(\"overwrite\").parquet(f\"{output_path}/office_sales_summary.parquet\")\n",
        "print(\"✅ Office sales summary saved to /output/processed/office_sales_summary.parquet\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iN0kWv_31E1k",
        "outputId": "ddd12ac1-8d78-442f-f013-7a60939b1e88"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------------+-------------+------------------+\n",
            "|officeCode|officeCity   |officeCountry|totalSales        |\n",
            "+----------+-------------+-------------+------------------+\n",
            "|4         |Paris        |France       |1607019.9899999995|\n",
            "|1         |San Francisco|USA          |1295933.3900000004|\n",
            "|7         |London       |UK           |932821.9999999999 |\n",
            "|6         |Sydney       |Australia    |870258.7599999999 |\n",
            "|3         |NYC          |USA          |866707.61         |\n",
            "|2         |Boston       |USA          |835882.3300000001 |\n",
            "|5         |Tokyo        |Japan        |457110.07000000007|\n",
            "+----------+-------------+-------------+------------------+\n",
            "\n",
            "✅ Office sales summary saved to /output/processed/office_sales_summary.parquet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, sum as _sum\n",
        "\n",
        "# Re-read data for demonstration\n",
        "orders_df = spark.read.parquet(\"/content/drive/MyDrive/classicmodels/orders\")\n",
        "orderdetails_df = spark.read.parquet(\"/content/drive/MyDrive/classicmodels/orderdetails\")\n",
        "\n",
        "# Join and calculate revenue\n",
        "order_data = orderdetails_df.join(orders_df, \"orderNumber\") \\\n",
        "    .withColumn(\"lineRevenue\", col(\"quantityOrdered\") * col(\"priceEach\"))\n",
        "\n",
        "# 🔁 Cache the DataFrame since it will be reused\n",
        "order_data.cache()\n",
        "\n",
        "# First use: total revenue per order\n",
        "order_total_df = order_data.groupBy(\"orderNumber\") \\\n",
        "    .agg(_sum(\"lineRevenue\").alias(\"orderRevenue\"))\n",
        "\n",
        "order_total_df.show()\n",
        "\n",
        "# Second use: total revenue per customer\n",
        "order_customer_df = order_data.groupBy(\"customerNumber\") \\\n",
        "    .agg(_sum(\"lineRevenue\").alias(\"customerRevenue\"))\n",
        "\n",
        "order_customer_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30DCMZVz2jZx",
        "outputId": "5c2e44eb-d1ea-446f-bd3c-9ef314eccaa1"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+------------------+\n",
            "|orderNumber|      orderRevenue|\n",
            "+-----------+------------------+\n",
            "|      10121|          16700.47|\n",
            "|      10128|          13884.99|\n",
            "|      10143| 41016.74999999999|\n",
            "|      10162|30876.439999999995|\n",
            "|      10116|           1627.56|\n",
            "|      10135|55601.840000000004|\n",
            "|      10124|32641.980000000003|\n",
            "|      10140|38675.130000000005|\n",
            "|      10131|          17032.29|\n",
            "|      10161|          36164.46|\n",
            "|      10136|           14232.7|\n",
            "|      10155|          37602.48|\n",
            "|      10138|32077.440000000002|\n",
            "|      10104|           40206.2|\n",
            "|      10117|          44380.15|\n",
            "|      10101|          10549.01|\n",
            "|      10114| 33383.14000000001|\n",
            "|      10147|          32680.31|\n",
            "|      10115|21665.980000000003|\n",
            "|      10108|51001.219999999994|\n",
            "+-----------+------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+--------------+------------------+\n",
            "|customerNumber|   customerRevenue|\n",
            "+--------------+------------------+\n",
            "|           148|           82730.3|\n",
            "|           496|32077.440000000002|\n",
            "|           458|          57131.92|\n",
            "|           321| 85559.11999999998|\n",
            "|           385|51001.219999999994|\n",
            "|           103|          14571.44|\n",
            "|           350|50824.659999999996|\n",
            "|           333|           9821.32|\n",
            "|           128|          10549.01|\n",
            "|           353|30620.730000000003|\n",
            "|           473|17746.260000000002|\n",
            "|           363|10223.829999999998|\n",
            "|           205|          50342.74|\n",
            "|           250|          23419.47|\n",
            "|           227|          36164.46|\n",
            "|           447|23663.649999999998|\n",
            "|           347|           20452.5|\n",
            "|           112|32641.980000000003|\n",
            "|           486|          25833.14|\n",
            "|           324|          29429.14|\n",
            "+--------------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import broadcast, col, sum as _sum\n",
        "\n",
        "# Load the datasets\n",
        "orderdetails_df = spark.read.parquet(\"/content/drive/MyDrive/classicmodels/orderdetails\")\n",
        "products_df = spark.read.parquet(\"/content/drive/MyDrive/classicmodels/products\")\n",
        "\n",
        "# ✅ Use broadcast join\n",
        "joined_df = orderdetails_df.join(broadcast(products_df), on=\"productCode\")\n",
        "\n",
        "# Aggregate total quantity sold per product\n",
        "product_sales_df = joined_df.groupBy(\"productName\") \\\n",
        "    .agg(_sum(\"quantityOrdered\").alias(\"totalQuantitySold\")) \\\n",
        "    .orderBy(col(\"totalQuantitySold\").desc())\n",
        "\n",
        "# Show top 10\n",
        "product_sales_df.show(10, truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdOhykGW2jcW",
        "outputId": "7ba2d2df-79b4-40ad-d642-558d1f07d60a"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------------------------+-----------------+\n",
            "|productName                          |totalQuantitySold|\n",
            "+-------------------------------------+-----------------+\n",
            "|1969 Harley Davidson Ultimate Chopper|1057             |\n",
            "|productName                          |null             |\n",
            "+-------------------------------------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import required modules\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, lit, coalesce\n",
        "\n",
        "# Step 2: Initialize SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"ClassicModels - Task 4.2 Caching Optimization\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# ✅ Step 3: Corrected paths to match your working directory\n",
        "input_base_path = \"/content/drive/MyDrive/classicmodels\"\n",
        "output_base_path = \"/content/drive/MyDrive/output/optimized\"\n",
        "\n",
        "# Step 4: Read the Parquet files\n",
        "orders_df = spark.read.parquet(f\"{input_base_path}/orders\")\n",
        "orderdetails_df = spark.read.parquet(f\"{input_base_path}/orderdetails\")\n",
        "products_df = spark.read.parquet(f\"{input_base_path}/products\")\n",
        "\n",
        "# Step 5: Join all 3 datasets\n",
        "joined_df = orders_df.join(orderdetails_df, on=\"orderNumber\", how=\"inner\") \\\n",
        "                     .join(products_df, on=\"productCode\", how=\"inner\")\n",
        "\n",
        "# Step 6: Cache the intermediate joined dataset\n",
        "joined_df.cache()\n",
        "\n",
        "# Step 7: Calculate revenue using coalesce to handle nulls\n",
        "revenue_df = joined_df.withColumn(\n",
        "    \"revenue\",\n",
        "    coalesce(col(\"priceEach\"), lit(0)) * coalesce(col(\"quantityOrdered\"), lit(0))\n",
        ")\n",
        "\n",
        "# Group by productName to get total revenue\n",
        "grouped_df = revenue_df.groupBy(\"productName\").sum(\"revenue\") \\\n",
        "                       .withColumnRenamed(\"sum(revenue)\", \"totalRevenue\")\n",
        "\n",
        "# Step 8: Display top 10 products by revenue\n",
        "grouped_df.orderBy(col(\"totalRevenue\").desc()).show(10, truncate=False)\n",
        "\n",
        "# Step 9: Save result to Parquet\n",
        "grouped_df.write.mode(\"overwrite\").parquet(f\"{output_base_path}/top_product_revenue.parquet\")\n",
        "print(f\"✅ Saved to: {output_base_path}/top_product_revenue.parquet\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5v5FouqR2jfU",
        "outputId": "81a26e2a-eec4-44ed-c534-aba2c1dc730c"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------------------------+------------------+\n",
            "|productName                          |totalRevenue      |\n",
            "+-------------------------------------+------------------+\n",
            "|1969 Harley Davidson Ultimate Chopper|16527.989999999998|\n",
            "+-------------------------------------+------------------+\n",
            "\n",
            "✅ Saved to: /content/drive/MyDrive/output/optimized/top_product_revenue.parquet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Step 1: Create Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Partitioning Optimization - ClassicModels\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Step 2: Define the correct base path (your mounted Google Drive location)\n",
        "base_path = \"/content/drive/MyDrive/classicmodels/\"\n",
        "\n",
        "# Step 3: Read Parquet datasets\n",
        "orders_df = spark.read.parquet(base_path + \"orders/\")\n",
        "orderdetails_df = spark.read.parquet(base_path + \"orderdetails/\")\n",
        "products_df = spark.read.parquet(base_path + \"products/\")\n",
        "\n",
        "# Step 4: Join orders with orderdetails and products\n",
        "order_with_details = orders_df.join(orderdetails_df, \"orderNumber\") \\\n",
        "                              .join(products_df, \"productCode\")\n",
        "\n",
        "# Step 5: Check current number of partitions\n",
        "print(f\"Original number of partitions: {order_with_details.rdd.getNumPartitions()}\")\n",
        "\n",
        "# Step 6: Repartition the data by product line (optimizing for grouped processing)\n",
        "repartitioned_df = order_with_details.repartition(\"productLine\")\n",
        "\n",
        "# Step 7: Save the repartitioned data to Parquet (to a new path)\n",
        "repartitioned_df.write.mode(\"overwrite\") \\\n",
        "    .parquet(base_path + \"output/repartitioned_by_productLine/\")\n",
        "\n",
        "print(\"✅ Repartitioned data saved to: /content/drive/MyDrive/classicmodels/output/repartitioned_by_productLine/\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AH2t9XTf2jiD",
        "outputId": "419cdb84-62c8-475f-a02d-9f3f2c103854"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original number of partitions: 1\n",
            "✅ Repartitioned data saved to: /content/drive/MyDrive/classicmodels/output/repartitioned_by_productLine/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive/classicmodels/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UI0f0JXS2jk-",
        "outputId": "33c4819a-daf8-495c-be2a-604b7f4d9be8"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "customers      offices\t\t orders        productlines\n",
            "customers.csv  offices.csv\t orders.csv    productlines.csv\n",
            "employees      orderdetails\t payments      products\n",
            "employees.csv  orderdetails.csv  payments.csv  products.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, sum as _sum\n",
        "\n",
        "# Input paths\n",
        "input_path = \"/content/drive/MyDrive/classicmodels\"\n",
        "output_path = \"/content/drive/MyDrive/classicmodels/output/processed\"\n",
        "\n",
        "# Load required datasets\n",
        "orders_df = spark.read.csv(f\"{input_path}/orders.csv\", header=True, inferSchema=True)\n",
        "orderdetails_df = spark.read.csv(f\"{input_path}/orderdetails.csv\", header=True, inferSchema=True)\n",
        "products_df = spark.read.csv(f\"{input_path}/products.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Join orderdetails with products\n",
        "product_orders = orderdetails_df.join(products_df, \"productCode\") \\\n",
        "                                .join(orders_df, \"orderNumber\")\n",
        "\n",
        "# Calculate revenue per product\n",
        "product_revenue_df = product_orders.withColumn(\"lineRevenue\", col(\"quantityOrdered\") * col(\"priceEach\")) \\\n",
        "    .groupBy(\"productCode\", \"productName\") \\\n",
        "    .agg(_sum(\"lineRevenue\").alias(\"totalRevenue\")) \\\n",
        "    .orderBy(col(\"totalRevenue\").desc())\n",
        "\n",
        "# Save to Parquet (correct path)\n",
        "product_revenue_df.write.mode(\"overwrite\").parquet(f\"{output_path}/product_revenue.parquet\")\n"
      ],
      "metadata": {
        "id": "28AH6s8_5zQu"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for file in output_files:\n",
        "    path = os.path.join(output_path, file)\n",
        "    if os.path.exists(path):\n",
        "        df = spark.read.parquet(path)\n",
        "        df.coalesce(1).write.mode(\"overwrite\").parquet(path)\n",
        "        print(f\"✔ Coalesced: {file}\")\n"
      ],
      "metadata": {
        "id": "poGNPDlr2jnf"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "customers_df = spark.read.csv(f\"{input_path}/customers.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Join orders + orderdetails\n",
        "order_data = orderdetails_df.join(orders_df, \"orderNumber\")\n",
        "\n",
        "# Calculate line item revenue\n",
        "order_data = order_data.withColumn(\"lineRevenue\", col(\"quantityOrdered\") * col(\"priceEach\"))\n",
        "\n",
        "# Total revenue per order\n",
        "order_total_df = order_data.groupBy(\"orderNumber\", \"customerNumber\") \\\n",
        "    .agg(_sum(\"lineRevenue\").alias(\"orderTotal\"))\n",
        "\n",
        "# Join with customers\n",
        "order_customer_df = order_total_df.join(customers_df, \"customerNumber\")\n",
        "\n",
        "# Group by customer and calculate avg order value\n",
        "from pyspark.sql.functions import round, count\n",
        "customer_avg_order_df = order_customer_df.groupBy(\"customerNumber\", \"customerName\") \\\n",
        "    .agg(round(_sum(\"orderTotal\") / count(\"orderNumber\"), 2).alias(\"avgOrderValue\")) \\\n",
        "    .orderBy(col(\"avgOrderValue\").desc())\n",
        "\n",
        "# Save output\n",
        "customer_avg_order_df.write.mode(\"overwrite\").parquet(f\"{output_path}/customer_avg_order.parquet\")\n",
        "\n"
      ],
      "metadata": {
        "id": "ejJXSsF05sm_"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_10_products_df = product_revenue_df.limit(10)\n",
        "\n",
        "# Save output\n",
        "top_10_products_df.write.mode(\"overwrite\").parquet(f\"{output_path}/top_10_products.parquet\")\n"
      ],
      "metadata": {
        "id": "7kgi6K-y5spx"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DiKKV_0J7VSi",
        "outputId": "48ae48f7-f546-43d8-b6d2-3b835287d23b"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Base path for ClassicModels CSV files\n",
        "base_path = \"/content/drive/MyDrive/classicmodels/\"\n",
        "\n",
        "# Output path to store processed Parquet files\n",
        "output_path = base_path + \"output/processed/\"\n",
        "\n",
        "# Create output folder if it does not exist\n",
        "import os\n",
        "os.makedirs(output_path, exist_ok=True)\n"
      ],
      "metadata": {
        "id": "uko52xMg5svf"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"ClassicModels Analysis\") \\\n",
        "    .getOrCreate()\n"
      ],
      "metadata": {
        "id": "R9AZucUy7Yyu"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "customers_df = spark.read.csv(base_path + \"customers.csv\", header=True, inferSchema=True)\n",
        "employees_df = spark.read.csv(base_path + \"employees.csv\", header=True, inferSchema=True)\n",
        "offices_df = spark.read.csv(base_path + \"offices.csv\", header=True, inferSchema=True)\n",
        "orders_df = spark.read.csv(base_path + \"orders.csv\", header=True, inferSchema=True)\n",
        "orderdetails_df = spark.read.csv(base_path + \"orderdetails.csv\", header=True, inferSchema=True)\n",
        "products_df = spark.read.csv(base_path + \"products.csv\", header=True, inferSchema=True)\n",
        "productlines_df = spark.read.csv(base_path + \"productlines.csv\", header=True, inferSchema=True)\n",
        "payments_df = spark.read.csv(base_path + \"payments.csv\", header=True, inferSchema=True)\n"
      ],
      "metadata": {
        "id": "SHzwgaUo7Y1A"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, sum as _sum\n",
        "\n",
        "# Join orderdetails + products + orders\n",
        "product_orders = orderdetails_df.join(products_df, \"productCode\") \\\n",
        "                                .join(orders_df, \"orderNumber\")\n",
        "\n",
        "# Calculate revenue per product\n",
        "product_revenue_df = product_orders.withColumn(\"lineRevenue\", col(\"quantityOrdered\") * col(\"priceEach\")) \\\n",
        "    .groupBy(\"productCode\", \"productName\") \\\n",
        "    .agg(_sum(\"lineRevenue\").alias(\"totalRevenue\")) \\\n",
        "    .orderBy(col(\"totalRevenue\").desc())\n",
        "\n",
        "# Save as Parquet\n",
        "product_revenue_df.write.mode(\"overwrite\").parquet(output_path + \"product_revenue.parquet\")\n"
      ],
      "metadata": {
        "id": "6ziFmHWQ7Y34"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import round, count\n",
        "\n",
        "# Calculate line revenue for each order detail\n",
        "order_data = orderdetails_df.join(orders_df, \"orderNumber\")\n",
        "order_data = order_data.withColumn(\"lineRevenue\", col(\"quantityOrdered\") * col(\"priceEach\"))\n",
        "\n",
        "# Total revenue per order\n",
        "order_total_df = order_data.groupBy(\"orderNumber\", \"customerNumber\") \\\n",
        "    .agg(_sum(\"lineRevenue\").alias(\"orderTotal\"))\n",
        "\n",
        "# Join with customers and calculate average order value\n",
        "customer_avg_order_df = order_total_df.join(customers_df, \"customerNumber\") \\\n",
        "    .groupBy(\"customerNumber\", \"customerName\") \\\n",
        "    .agg(round(_sum(\"orderTotal\") / count(\"orderNumber\"), 2).alias(\"avgOrderValue\")) \\\n",
        "    .orderBy(col(\"avgOrderValue\").desc())\n",
        "\n",
        "# Save as Parquet\n",
        "customer_avg_order_df.write.mode(\"overwrite\").parquet(output_path + \"customer_avg_order.parquet\")\n"
      ],
      "metadata": {
        "id": "nIV99sMa7Y6o"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_10_products_df = product_revenue_df.limit(10)\n",
        "top_10_products_df.write.mode(\"overwrite\").parquet(output_path + \"top_10_products.parquet\")\n"
      ],
      "metadata": {
        "id": "AYK8Cuvw7Y84"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Orders columns:\", orders_df.columns)\n",
        "print(\"Employees columns:\", employees_df.columns)\n",
        "print(\"Offices columns:\", offices_df.columns)\n",
        "print(\"OrderDetails columns:\", orderdetails_df.columns)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KFJsBAw07Y_U",
        "outputId": "29098927-6023-4594-e9f9-c60b594b5fbe"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Orders columns: ['orderNumber', 'orderDate', 'requiredDate', 'shippedDate', 'status', 'comments', 'customerNumber']\n",
            "Employees columns: ['employeeNumber', 'lastName', 'firstName', 'extension', 'email', 'officeCode', 'reportsTo', 'jobTitle']\n",
            "Offices columns: ['officeCode', 'city', 'phone', 'addressLine1', 'addressLine2', 'state', 'country', 'postalCode', 'territory']\n",
            "OrderDetails columns: ['orderNumber', 'productCode', 'quantityOrdered', 'priceEach', 'orderLineNumber']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Mount Google Drive (if not already mounted)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Step 2: Import required libraries\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, sum as _sum, avg, desc\n",
        "\n",
        "# Step 3: Create Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"ClassicModels Analysis\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Step 4: Define base path\n",
        "base_path = \"/content/drive/MyDrive/classicmodels/\"\n",
        "\n",
        "# Step 5: Read CSV files into DataFrames\n",
        "orders_df = spark.read.option(\"header\", True).csv(base_path + \"orders.csv\")\n",
        "orderdetails_df = spark.read.option(\"header\", True).csv(base_path + \"orderdetails.csv\")\n",
        "products_df = spark.read.option(\"header\", True).csv(base_path + \"products.csv\")\n",
        "customers_df = spark.read.option(\"header\", True).csv(base_path + \"customers.csv\")\n",
        "# employees_df and offices_df are read but we won't use them (no salesRepEmployeeNumber)\n",
        "employees_df = spark.read.option(\"header\", True).csv(base_path + \"employees.csv\")\n",
        "offices_df = spark.read.option(\"header\", True).csv(base_path + \"offices.csv\")\n",
        "\n",
        "# Step 6: Cast numeric columns\n",
        "orderdetails_df = orderdetails_df.withColumn(\"quantityOrdered\", col(\"quantityOrdered\").cast(\"int\")) \\\n",
        "                                 .withColumn(\"priceEach\", col(\"priceEach\").cast(\"double\"))\n",
        "\n",
        "# Step 7: Product-wise revenue\n",
        "product_revenue_df = orderdetails_df.join(products_df, \"productCode\") \\\n",
        "    .withColumn(\"totalRevenue\", col(\"quantityOrdered\") * col(\"priceEach\")) \\\n",
        "    .groupBy(\"productCode\", \"productName\") \\\n",
        "    .agg(_sum(\"totalRevenue\").alias(\"totalRevenue\")) \\\n",
        "    .orderBy(desc(\"totalRevenue\"))\n",
        "\n",
        "# Save to Parquet\n",
        "product_revenue_df.write.mode(\"overwrite\").parquet(base_path + \"product_revenue.parquet\")\n",
        "\n",
        "# Step 8: Average order value per customer\n",
        "customer_order_df = orderdetails_df.join(orders_df, \"orderNumber\") \\\n",
        "    .withColumn(\"orderValue\", col(\"quantityOrdered\") * col(\"priceEach\")) \\\n",
        "    .groupBy(\"customerNumber\") \\\n",
        "    .agg(avg(\"orderValue\").alias(\"avgOrderValue\"))\n",
        "\n",
        "# Join with customer name\n",
        "customer_avg_order_df = customer_order_df.join(customers_df, \"customerNumber\") \\\n",
        "    .select(\"customerNumber\", \"customerName\", \"avgOrderValue\") \\\n",
        "    .orderBy(desc(\"avgOrderValue\"))\n",
        "\n",
        "# Save to Parquet\n",
        "customer_avg_order_df.write.mode(\"overwrite\").parquet(base_path + \"customer_avg_order.parquet\")\n",
        "\n",
        "# Step 9: Top 10 products by revenue\n",
        "top_10_products_df = product_revenue_df.limit(10)\n",
        "top_10_products_df.write.mode(\"overwrite\").parquet(base_path + \"top_10_products.parquet\")\n",
        "\n",
        "# Step 10: Regional sales (by customer country)\n",
        "regional_sales_df = orderdetails_df.join(orders_df, \"orderNumber\") \\\n",
        "    .join(customers_df, \"customerNumber\") \\\n",
        "    .withColumn(\"totalSale\", col(\"quantityOrdered\") * col(\"priceEach\")) \\\n",
        "    .groupBy(\"country\") \\\n",
        "    .agg(_sum(\"totalSale\").alias(\"totalRevenue\")) \\\n",
        "    .orderBy(desc(\"totalRevenue\"))\n",
        "\n",
        "regional_sales_df.write.mode(\"overwrite\").parquet(base_path + \"regional_sales.parquet\")\n",
        "\n",
        "print(\"✅ All outputs saved to Parquet!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bHLfPU37ZBg",
        "outputId": "c75deb6d-6031-4808-8ad5-a4ce7fb94ea3"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "✅ All outputs saved to Parquet!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Step 2: Import libraries\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, sum as _sum, avg, desc\n",
        "\n",
        "# Step 3: Create Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"ClassicModels Analysis\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Step 4: Define base path\n",
        "base_path = \"/content/drive/MyDrive/classicmodels/\"\n",
        "\n",
        "# Step 5: Read CSV files\n",
        "orders_df = spark.read.option(\"header\", True).csv(base_path + \"orders.csv\")\n",
        "orderdetails_df = spark.read.option(\"header\", True).csv(base_path + \"orderdetails.csv\")\n",
        "products_df = spark.read.option(\"header\", True).csv(base_path + \"products.csv\")\n",
        "customers_df = spark.read.option(\"header\", True).csv(base_path + \"customers.csv\")\n",
        "\n",
        "# Step 6: Cast numeric columns\n",
        "orderdetails_df = orderdetails_df.withColumn(\"quantityOrdered\", col(\"quantityOrdered\").cast(\"int\")) \\\n",
        "                                 .withColumn(\"priceEach\", col(\"priceEach\").cast(\"double\"))\n",
        "\n",
        "# Step 7: Product-wise revenue\n",
        "product_revenue_df = orderdetails_df.join(products_df, \"productCode\") \\\n",
        "    .withColumn(\"totalRevenue\", col(\"quantityOrdered\") * col(\"priceEach\")) \\\n",
        "    .groupBy(\"productCode\", \"productName\") \\\n",
        "    .agg(_sum(\"totalRevenue\").alias(\"totalRevenue\")) \\\n",
        "    .orderBy(desc(\"totalRevenue\"))\n",
        "\n",
        "product_revenue_df.write.mode(\"overwrite\").parquet(base_path + \"product_revenue.parquet\")\n",
        "print(\"Top 5 Product Revenue:\")\n",
        "product_revenue_df.show(5, truncate=False)\n",
        "\n",
        "# Step 8: Average order value per customer\n",
        "customer_order_df = orderdetails_df.join(orders_df, \"orderNumber\") \\\n",
        "    .withColumn(\"orderValue\", col(\"quantityOrdered\") * col(\"priceEach\")) \\\n",
        "    .groupBy(\"customerNumber\") \\\n",
        "    .agg(avg(\"orderValue\").alias(\"avgOrderValue\"))\n",
        "\n",
        "customer_avg_order_df = customer_order_df.join(customers_df, \"customerNumber\") \\\n",
        "    .select(\"customerNumber\", \"customerName\", \"avgOrderValue\") \\\n",
        "    .orderBy(desc(\"avgOrderValue\"))\n",
        "\n",
        "customer_avg_order_df.write.mode(\"overwrite\").parquet(base_path + \"customer_avg_order.parquet\")\n",
        "print(\"Top 5 Customers by Avg Order Value:\")\n",
        "customer_avg_order_df.show(5, truncate=False)\n",
        "\n",
        "# Step 9: Top 10 products by revenue\n",
        "top_10_products_df = product_revenue_df.limit(10)\n",
        "top_10_products_df.write.mode(\"overwrite\").parquet(base_path + \"top_10_products.parquet\")\n",
        "print(\"Top 10 Products by Revenue:\")\n",
        "top_10_products_df.show(10, truncate=False)\n",
        "\n",
        "# Step 10: Regional sales by country\n",
        "regional_sales_df = orderdetails_df.join(orders_df, \"orderNumber\") \\\n",
        "    .join(customers_df, \"customerNumber\") \\\n",
        "    .withColumn(\"totalSale\", col(\"quantityOrdered\") * col(\"priceEach\")) \\\n",
        "    .groupBy(\"country\") \\\n",
        "    .agg(_sum(\"totalSale\").alias(\"totalRevenue\")) \\\n",
        "    .orderBy(desc(\"totalRevenue\"))\n",
        "\n",
        "regional_sales_df.write.mode(\"overwrite\").parquet(base_path + \"regional_sales.parquet\")\n",
        "print(\"Regional Sales by Country (Top 5):\")\n",
        "regional_sales_df.show(5, truncate=False)\n",
        "\n",
        "print(\"✅ All outputs saved and verified!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YlsEykZv7ZFr",
        "outputId": "7af3a8ca-7b27-4b20-8040-2b7498d0a054"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Top 5 Product Revenue:\n",
            "+-----------+-------------------------------------+-----------------+\n",
            "|productCode|productName                          |totalRevenue     |\n",
            "+-----------+-------------------------------------+-----------------+\n",
            "|S10_1678   |1969 Harley Davidson Ultimate Chopper|90157.77000000002|\n",
            "|productCode|productName                          |null             |\n",
            "+-----------+-------------------------------------+-----------------+\n",
            "\n",
            "Top 5 Customers by Avg Order Value:\n",
            "+--------------+----------------------------+------------------+\n",
            "|customerNumber|customerName                |avgOrderValue     |\n",
            "+--------------+----------------------------+------------------+\n",
            "|242           |Alpha Cognac                |4744.233333333334 |\n",
            "|486           |Motor Mint Distributors Inc.|4305.5233333333335|\n",
            "|424           |Classic Legends Inc.        |3973.4863636363643|\n",
            "|151           |Muscle Machine Inc          |3922.7566666666667|\n",
            "|103           |Atelier graphique           |3642.86           |\n",
            "+--------------+----------------------------+------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Top 10 Products by Revenue:\n",
            "+-----------+-------------------------------------+-----------------+\n",
            "|productCode|productName                          |totalRevenue     |\n",
            "+-----------+-------------------------------------+-----------------+\n",
            "|S10_1678   |1969 Harley Davidson Ultimate Chopper|90157.77000000002|\n",
            "|productCode|productName                          |null             |\n",
            "+-----------+-------------------------------------+-----------------+\n",
            "\n",
            "Regional Sales by Country (Top 5):\n",
            "+---------+-----------------+\n",
            "|country  |totalRevenue     |\n",
            "+---------+-----------------+\n",
            "|USA      |650710.1999999998|\n",
            "|France   |133669.0         |\n",
            "|Finland  |100042.38        |\n",
            "|Denmark  |90123.66999999998|\n",
            "|Australia|87263.95         |\n",
            "+---------+-----------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "✅ All outputs saved and verified!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# Step 1: Import PySpark\n",
        "# -------------------------------\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, sum as _sum, avg as _avg, round as _round, count as _count, countDistinct as _countDistinct\n",
        "\n",
        "# -------------------------------\n",
        "# Step 2: Initialize Spark Session\n",
        "# -------------------------------\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"ClassicModelsSalesAnalysis\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# -------------------------------\n",
        "# Step 3: Define paths\n",
        "# -------------------------------\n",
        "base_path = \"/content/drive/MyDrive/classicmodels/\"\n",
        "output_path = \"/content/drive/MyDrive/classicmodels/output/processed/\"\n",
        "\n",
        "# -------------------------------\n",
        "# Step 4: Read CSVs into DataFrames\n",
        "# -------------------------------\n",
        "customers_df = spark.read.option(\"header\", True).csv(base_path + \"customers.csv\")\n",
        "employees_df = spark.read.option(\"header\", True).csv(base_path + \"employees.csv\")\n",
        "offices_df = spark.read.option(\"header\", True).csv(base_path + \"offices.csv\")\n",
        "orders_df = spark.read.option(\"header\", True).csv(base_path + \"orders.csv\")\n",
        "orderdetails_df = spark.read.option(\"header\", True).csv(base_path + \"orderdetails.csv\")\n",
        "products_df = spark.read.option(\"header\", True).csv(base_path + \"products.csv\")\n",
        "payments_df = spark.read.option(\"header\", True).csv(base_path + \"payments.csv\")\n",
        "\n",
        "# -------------------------------\n",
        "# Step 5: Cast numeric columns\n",
        "# -------------------------------\n",
        "orderdetails_df = orderdetails_df.withColumn(\"quantityOrdered\", col(\"quantityOrdered\").cast(\"int\")) \\\n",
        "                                 .withColumn(\"priceEach\", col(\"priceEach\").cast(\"double\"))\n",
        "\n",
        "products_df = products_df.withColumn(\"MSRP\", col(\"MSRP\").cast(\"double\"))\n",
        "\n",
        "payments_df = payments_df.withColumn(\"amount\", col(\"amount\").cast(\"double\"))\n",
        "\n",
        "# -------------------------------\n",
        "# Step 6: Task 1 - Products Aggregation\n",
        "# -------------------------------\n",
        "products_summary_df = products_df.groupBy(\"productLine\") \\\n",
        "    .agg(\n",
        "        _count(\"productCode\").alias(\"totalProducts\"),\n",
        "        _round(_avg(\"MSRP\"),2).alias(\"avgMSRP\"),\n",
        "        _round(_max(\"MSRP\"),2).alias(\"maxMSRP\")\n",
        "    )\n",
        "products_summary_df.write.mode(\"overwrite\").parquet(output_path + \"products_summary.parquet\")\n",
        "\n",
        "# -------------------------------\n",
        "# Step 7: Task 2 - Product & Order Analysis\n",
        "# -------------------------------\n",
        "# 7a: Top 10 Products by Quantity Sold\n",
        "top_10_products_df = orderdetails_df.join(products_df, \"productCode\") \\\n",
        "    .groupBy(\"productCode\", \"productName\") \\\n",
        "    .agg(_sum(\"quantityOrdered\").alias(\"totalQuantity\")) \\\n",
        "    .orderBy(col(\"totalQuantity\").desc()) \\\n",
        "    .limit(10)\n",
        "top_10_products_df.write.mode(\"overwrite\").parquet(output_path + \"top_10_products.parquet\")\n",
        "\n",
        "# 7b: Product-wise Revenue\n",
        "product_revenue_df = orderdetails_df.join(products_df, \"productCode\") \\\n",
        "    .withColumn(\"revenue\", col(\"quantityOrdered\") * col(\"priceEach\")) \\\n",
        "    .groupBy(\"productCode\", \"productName\") \\\n",
        "    .agg(_round(_sum(\"revenue\"),2).alias(\"totalRevenue\"))\n",
        "product_revenue_df.write.mode(\"overwrite\").parquet(output_path + \"product_revenue.parquet\")\n",
        "\n",
        "# 7c: Average Order Value per Customer\n",
        "customer_order_df = orders_df.join(orderdetails_df, \"orderNumber\") \\\n",
        "    .join(customers_df, \"customerNumber\") \\\n",
        "    .withColumn(\"orderRevenue\", col(\"quantityOrdered\") * col(\"priceEach\"))\n",
        "\n",
        "customer_avg_order_df = customer_order_df.groupBy(\"customerNumber\", \"customerName\") \\\n",
        "    .agg(_round(_sum(\"orderRevenue\")/_countDistinct(\"orderNumber\"),2).alias(\"avgOrderValue\"))\n",
        "customer_avg_order_df.write.mode(\"overwrite\").parquet(output_path + \"customer_avg_order.parquet\")\n",
        "\n",
        "# -------------------------------\n",
        "# Step 8: Task 3 - Regional & Office Sales\n",
        "# -------------------------------\n",
        "# Join orders -> orderdetails -> customers -> employees -> offices\n",
        "# Join orders -> orderdetails -> customers -> employees -> offices\n",
        "sales_df = orders_df.join(orderdetails_df, \"orderNumber\") \\\n",
        "    .join(customers_df, \"customerNumber\") \\\n",
        "    .join(employees_df, customers_df[\"salesRepEmployeeNumber\"] == employees_df[\"employeeNumber\"]) \\\n",
        "    .join(offices_df, employees_df[\"officeCode\"] == offices_df[\"officeCode\"]) \\\n",
        "    .withColumn(\"revenue\", col(\"quantityOrdered\") * col(\"priceEach\")) \\\n",
        "    .select(\n",
        "        orders_df[\"orderNumber\"],\n",
        "        customers_df[\"customerNumber\"],\n",
        "        customers_df[\"customerName\"],\n",
        "        employees_df[\"employeeNumber\"],\n",
        "        employees_df[\"lastName\"].alias(\"employeeLastName\"),\n",
        "        offices_df[\"officeCode\"],\n",
        "        offices_df[\"city\"].alias(\"officeCity\"),\n",
        "        offices_df[\"country\"].alias(\"officeCountry\"),\n",
        "        col(\"revenue\")\n",
        "    )\n",
        "\n",
        "# 8a: Revenue by Region/Country\n",
        "regional_sales_df = sales_df.groupBy(\"officeCountry\") \\\n",
        "    .agg(_round(_sum(\"revenue\"),2).alias(\"totalRevenue\"))\n",
        "regional_sales_df.write.mode(\"overwrite\").parquet(output_path + \"regional_sales.parquet\")\n",
        "\n",
        "# 8b: Revenue by Office\n",
        "office_sales_df = sales_df.groupBy(\"officeCode\", \"officeCity\") \\\n",
        "    .agg(_round(_sum(\"revenue\"),2).alias(\"totalRevenue\"))\n",
        "office_sales_df.write.mode(\"overwrite\").parquet(output_path + \"office_sales.parquet\")\n",
        "\n",
        "# -------------------------------\n",
        "print(\"All tasks completed successfully! Outputs are in:\", output_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0yMr8Qr_8oRD",
        "outputId": "3312ad98-e3e0-40f7-f7cc-a7d61167dd1d"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All tasks completed successfully! Outputs are in: /content/drive/MyDrive/classicmodels/output/processed/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display Products Summary\n",
        "products_summary_df.show(truncate=False)\n",
        "\n",
        "# Display Customer Average Order\n",
        "customer_avg_order_df.show(truncate=False)\n",
        "\n",
        "# Display Top 10 Products by Revenue\n",
        "top_10_products_df.show(truncate=False)\n",
        "\n",
        "# Display Regional Sales\n",
        "regional_sales_df.show(truncate=False)\n",
        "\n",
        "# Display Office Sales\n",
        "office_sales_df.show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1nRUVz0t8oTx",
        "outputId": "04f44840-4eec-4050-9935-cf2f0a050cfc"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+-------+-------+\n",
            "|productLine|totalProducts|avgMSRP|maxMSRP|\n",
            "+-----------+-------------+-------+-------+\n",
            "|productLine|1            |null   |null   |\n",
            "|Motorcycles|1            |95.7   |95.7   |\n",
            "+-----------+-------------+-------+-------+\n",
            "\n",
            "+--------------+----------------------------+-------------+\n",
            "|customerNumber|customerName                |avgOrderValue|\n",
            "+--------------+----------------------------+-------------+\n",
            "|customerNumber|customerName                |null         |\n",
            "|114           |Australian Collectors, Co.  |26714.56     |\n",
            "|321           |Corporate Gift Ideas Co.    |42779.56     |\n",
            "|205           |Toys4GrownUps.com           |50342.74     |\n",
            "|103           |Atelier graphique           |14571.44     |\n",
            "|278           |Rovelli Gifts               |52151.81     |\n",
            "|385           |Cruz & Sons Co.             |51001.22     |\n",
            "|382           |Salzburg Collectables       |35826.33     |\n",
            "|198           |Auto-Moto Classics Inc.     |6036.96      |\n",
            "|311           |Oulu Toy Supplies, Inc.     |32723.04     |\n",
            "|128           |Blauer See Auto, Co.        |10549.01     |\n",
            "|486           |Motor Mint Distributors Inc.|25833.14     |\n",
            "|447           |Gift Ideas Corp.            |11831.83     |\n",
            "|323           |Down Under Souveniers, Inc  |2880.0       |\n",
            "|124           |Mini Gifts Distributors Ltd.|40899.57     |\n",
            "|148           |Dragon Souveniers, Ltd.     |41365.15     |\n",
            "|145           |Danish Wholesale Imports    |53959.21     |\n",
            "|129           |Mini Wheels Co.             |16537.85     |\n",
            "|181           |Vitachrome Inc.             |5494.78      |\n",
            "|112           |Signal Gift Stores          |32641.98     |\n",
            "+--------------+----------------------------+-------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+-----------+-------------------------------------+-------------+\n",
            "|productCode|productName                          |totalQuantity|\n",
            "+-----------+-------------------------------------+-------------+\n",
            "|S10_1678   |1969 Harley Davidson Ultimate Chopper|1057         |\n",
            "|productCode|productName                          |null         |\n",
            "+-----------+-------------------------------------+-------------+\n",
            "\n",
            "+-------------+------------+\n",
            "|officeCountry|totalRevenue|\n",
            "+-------------+------------+\n",
            "|UK           |162301.72   |\n",
            "|Australia    |122221.39   |\n",
            "|Japan        |133731.52   |\n",
            "|USA          |650710.2    |\n",
            "|France       |329517.07   |\n",
            "+-------------+------------+\n",
            "\n",
            "+----------+-------------+------------+\n",
            "|officeCode|officeCity   |totalRevenue|\n",
            "+----------+-------------+------------+\n",
            "|3         |NYC          |179833.89   |\n",
            "|5         |Tokyo        |133731.52   |\n",
            "|6         |Sydney       |122221.39   |\n",
            "|2         |Boston       |89957.85    |\n",
            "|7         |London       |162301.72   |\n",
            "|4         |Paris        |329517.07   |\n",
            "|1         |San Francisco|380918.46   |\n",
            "+----------+-------------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import Libraries\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, sum as _sum, avg as _avg, count as _count, round as _round\n",
        "\n",
        "# Step 2: Initialize Spark\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"ClassicModels Analysis\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Step 3: File paths\n",
        "base_path = \"/content/drive/MyDrive/classicmodels/\"\n",
        "output_path = \"/content/drive/MyDrive/classicmodels/output/processed/\"\n",
        "\n",
        "# Step 4: Read CSVs\n",
        "customers_df = spark.read.option(\"header\",True).csv(base_path + \"customers.csv\")\n",
        "employees_df = spark.read.option(\"header\",True).csv(base_path + \"employees.csv\")\n",
        "offices_df = spark.read.option(\"header\",True).csv(base_path + \"offices.csv\")\n",
        "orders_df = spark.read.option(\"header\",True).csv(base_path + \"orders.csv\")\n",
        "orderdetails_df = spark.read.option(\"header\",True).csv(base_path + \"orderdetails.csv\")\n",
        "products_df = spark.read.option(\"header\",True).csv(base_path + \"products.csv\")\n",
        "productlines_df = spark.read.option(\"header\",True).csv(base_path + \"productlines.csv\")\n",
        "payments_df = spark.read.option(\"header\",True).csv(base_path + \"payments.csv\")\n",
        "\n",
        "# Step 5: Cast columns to proper types\n",
        "orderdetails_df = orderdetails_df.withColumn(\"quantityOrdered\", col(\"quantityOrdered\").cast(\"int\")) \\\n",
        "                                 .withColumn(\"priceEach\", col(\"priceEach\").cast(\"double\"))\n",
        "products_df = products_df.withColumn(\"MSRP\", col(\"MSRP\").cast(\"double\"))\n",
        "payments_df = payments_df.withColumn(\"amount\", col(\"amount\").cast(\"double\"))\n",
        "\n",
        "# Step 6: Convert CSVs to Parquet\n",
        "customers_df.write.mode(\"overwrite\").parquet(base_path + \"customers.parquet\")\n",
        "employees_df.write.mode(\"overwrite\").parquet(base_path + \"employees.parquet\")\n",
        "offices_df.write.mode(\"overwrite\").parquet(base_path + \"offices.parquet\")\n",
        "orders_df.write.mode(\"overwrite\").parquet(base_path + \"orders.parquet\")\n",
        "orderdetails_df.write.mode(\"overwrite\").parquet(base_path + \"orderdetails.parquet\")\n",
        "products_df.write.mode(\"overwrite\").parquet(base_path + \"products.parquet\")\n",
        "productlines_df.write.mode(\"overwrite\").parquet(base_path + \"productlines.parquet\")\n",
        "payments_df.write.mode(\"overwrite\").parquet(base_path + \"payments.parquet\")\n",
        "\n",
        "# -------------------------------\n",
        "# Task 2: Product & Order Analysis\n",
        "# -------------------------------\n",
        "\n",
        "# 2a: Top 10 products by quantity sold\n",
        "top_10_products_df = orderdetails_df.groupBy(\"productCode\") \\\n",
        "    .agg(_sum(\"quantityOrdered\").alias(\"totalQuantity\")) \\\n",
        "    .join(products_df.select(\"productCode\",\"productName\"), \"productCode\") \\\n",
        "    .orderBy(col(\"totalQuantity\").desc()) \\\n",
        "    .limit(10)\n",
        "\n",
        "top_10_products_df.write.mode(\"overwrite\").parquet(output_path + \"top_10_products.parquet\")\n",
        "\n",
        "# 2b: Product-wise revenue\n",
        "product_revenue_df = orderdetails_df.join(products_df, \"productCode\") \\\n",
        "    .withColumn(\"revenue\", col(\"quantityOrdered\") * col(\"priceEach\")) \\\n",
        "    .groupBy(\"productCode\",\"productName\") \\\n",
        "    .agg(_round(_sum(\"revenue\"),2).alias(\"totalRevenue\"))\n",
        "\n",
        "product_revenue_df.write.mode(\"overwrite\").parquet(output_path + \"product_revenue.parquet\")\n",
        "\n",
        "# 2c: Average order value per customer\n",
        "customer_order_df = orders_df.join(orderdetails_df, \"orderNumber\") \\\n",
        "                             .join(customers_df.select(\"customerNumber\",\"customerName\"), \"customerNumber\") \\\n",
        "                             .withColumn(\"orderValue\", col(\"quantityOrdered\") * col(\"priceEach\"))\n",
        "\n",
        "customer_avg_order_df = customer_order_df.groupBy(\"customerNumber\",\"customerName\") \\\n",
        "    .agg(_round(_sum(\"orderValue\") / _count(\"orderNumber\"),2).alias(\"avgOrderValue\"))\n",
        "\n",
        "customer_avg_order_df.write.mode(\"overwrite\").parquet(output_path + \"customer_avg_order.parquet\")\n",
        "\n",
        "# -------------------------------\n",
        "# Task 3: Regional Sales Insights\n",
        "# -------------------------------\n",
        "\n",
        "# Join orders -> orderdetails -> customers -> offices via employees if salesRep exists\n",
        "# Since your orders CSV has no salesRepEmployeeNumber, we join only orders -> customers -> offices\n",
        "sales_df = orders_df.join(orderdetails_df, \"orderNumber\") \\\n",
        "                    .join(customers_df.select(\"customerNumber\",\"customerName\",\"country\",\"salesRepEmployeeNumber\"), \"customerNumber\") \\\n",
        "                    .join(offices_df.select(\"officeCode\",\"city\",\"country\").withColumnRenamed(\"country\",\"officeCountry\"), orders_df.customerNumber.isNotNull(), \"left\") \\\n",
        "                    .withColumn(\"revenue\", col(\"quantityOrdered\") * col(\"priceEach\"))\n",
        "\n",
        "# Revenue by country\n",
        "regional_sales_df = sales_df.groupBy(\"country\").agg(_round(_sum(\"revenue\"),2).alias(\"totalRevenue\"))\n",
        "regional_sales_df.write.mode(\"overwrite\").parquet(output_path + \"regional_sales.parquet\")\n",
        "\n",
        "# Revenue by office\n",
        "office_sales_df = sales_df.groupBy(\"officeCode\",\"city\").agg(_round(_sum(\"revenue\"),2).alias(\"totalRevenue\"))\n",
        "office_sales_df.write.mode(\"overwrite\").parquet(output_path + \"office_sales.parquet\")\n",
        "\n",
        "# -------------------------------\n",
        "# Task 4: Product Demand & Employee Metrics\n",
        "# -------------------------------\n",
        "\n",
        "# Products demand summary\n",
        "product_demand_df = orderdetails_df.join(products_df, \"productCode\") \\\n",
        "    .groupBy(\"productCode\",\"productName\") \\\n",
        "    .agg(_count(\"orderNumber\").alias(\"orderCount\"),\n",
        "         _sum(\"quantityOrdered\").alias(\"totalOrderedQty\"),\n",
        "         _round(_avg(\"priceEach\"),2).alias(\"averagePriceEach\"))\n",
        "\n",
        "product_demand_df.write.mode(\"overwrite\").parquet(output_path + \"product_demand_summary.parquet\")\n",
        "\n",
        "# Employee performance summary (only employees linked in customers)\n",
        "employee_sales_df = orders_df.join(customers_df, orders_df.customerNumber == customers_df.customerNumber, \"inner\") \\\n",
        "                             .join(orderdetails_df, \"orderNumber\") \\\n",
        "                             .groupBy(\"salesRepEmployeeNumber\") \\\n",
        "                             .agg(_count(\"orderNumber\").alias(\"totalOrders\"),\n",
        "                                  _round(_sum(col(\"quantityOrdered\")*col(\"priceEach\")),2).alias(\"totalSales\"))\n",
        "\n",
        "employee_sales_df.write.mode(\"overwrite\").parquet(output_path + \"employee_sales_summary.parquet\")\n",
        "\n",
        "# -------------------------------\n",
        "# Task 5: Products Aggregation\n",
        "# -------------------------------\n",
        "\n",
        "products_summary_df = products_df.groupBy(\"productLine\") \\\n",
        "    .agg(_count(\"productCode\").alias(\"totalProducts\"),\n",
        "         _round(_avg(\"MSRP\"),2).alias(\"avgMSRP\"),\n",
        "         _round(_max(\"MSRP\"),2).alias(\"maxMSRP\"))\n",
        "\n",
        "products_summary_df.write.mode(\"overwrite\").parquet(output_path + \"products_summary.parquet\")\n",
        "\n",
        "print(\"✅ All tasks completed. Parquet outputs are in:\", output_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEBUJGJ38oWP",
        "outputId": "b3500f60-6f9a-410d-8a24-b1d09574e672"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ All tasks completed. Parquet outputs are in: /content/drive/MyDrive/classicmodels/output/processed/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Output folder\n",
        "output_path = \"/content/drive/MyDrive/classicmodels/output/processed/\"\n",
        "\n",
        "# Step 2: Read and display all outputs\n",
        "\n",
        "print(\"---- Top 10 Products ----\")\n",
        "top_10_products = spark.read.parquet(output_path + \"top_10_products.parquet\")\n",
        "top_10_products.show(truncate=False)\n",
        "\n",
        "print(\"---- Product Revenue ----\")\n",
        "product_revenue = spark.read.parquet(output_path + \"product_revenue.parquet\")\n",
        "product_revenue.show(truncate=False)\n",
        "\n",
        "print(\"---- Customer Average Order ----\")\n",
        "customer_avg_order = spark.read.parquet(output_path + \"customer_avg_order.parquet\")\n",
        "customer_avg_order.show(truncate=False)\n",
        "\n",
        "print(\"---- Regional Sales ----\")\n",
        "regional_sales = spark.read.parquet(output_path + \"regional_sales.parquet\")\n",
        "regional_sales.show(truncate=False)\n",
        "\n",
        "print(\"---- Office Sales ----\")\n",
        "office_sales = spark.read.parquet(output_path + \"office_sales.parquet\")\n",
        "office_sales.show(truncate=False)\n",
        "\n",
        "print(\"---- Product Demand Summary ----\")\n",
        "product_demand_summary = spark.read.parquet(output_path + \"product_demand_summary.parquet\")\n",
        "product_demand_summary.show(truncate=False)\n",
        "\n",
        "print(\"---- Employee Sales Summary ----\")\n",
        "employee_sales_summary = spark.read.parquet(output_path + \"employee_sales_summary.parquet\")\n",
        "employee_sales_summary.show(truncate=False)\n",
        "\n",
        "print(\"---- Products Summary ----\")\n",
        "products_summary = spark.read.parquet(output_path + \"products_summary.parquet\")\n",
        "products_summary.show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZk43uYT8oYv",
        "outputId": "6a486334-93d2-4c07-c7a3-adb9a0781a26"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---- Top 10 Products ----\n",
            "+-----------+-------------+-------------------------------------+\n",
            "|productCode|totalQuantity|productName                          |\n",
            "+-----------+-------------+-------------------------------------+\n",
            "|S10_1678   |1057         |1969 Harley Davidson Ultimate Chopper|\n",
            "|productCode|null         |productName                          |\n",
            "+-----------+-------------+-------------------------------------+\n",
            "\n",
            "---- Product Revenue ----\n",
            "+-----------+-------------------------------------+------------+\n",
            "|productCode|productName                          |totalRevenue|\n",
            "+-----------+-------------------------------------+------------+\n",
            "|productCode|productName                          |null        |\n",
            "|S10_1678   |1969 Harley Davidson Ultimate Chopper|90157.77    |\n",
            "+-----------+-------------------------------------+------------+\n",
            "\n",
            "---- Customer Average Order ----\n",
            "+--------------+----------------------------+-------------+\n",
            "|customerNumber|customerName                |avgOrderValue|\n",
            "+--------------+----------------------------+-------------+\n",
            "|customerNumber|customerName                |null         |\n",
            "|278           |Rovelli Gifts               |2897.32      |\n",
            "|385           |Cruz & Sons Co.             |3187.58      |\n",
            "|382           |Salzburg Collectables       |2559.02      |\n",
            "|114           |Australian Collectors, Co.  |3142.89      |\n",
            "|103           |Atelier graphique           |3642.86      |\n",
            "|198           |Auto-Moto Classics Inc.     |3018.48      |\n",
            "|205           |Toys4GrownUps.com           |3146.42      |\n",
            "|311           |Oulu Toy Supplies, Inc.     |3272.3       |\n",
            "|321           |Corporate Gift Ideas Co.    |3055.68      |\n",
            "|128           |Blauer See Auto, Co.        |2637.25      |\n",
            "|145           |Danish Wholesale Imports    |3597.28      |\n",
            "|486           |Motor Mint Distributors Inc.|4305.52      |\n",
            "|129           |Mini Wheels Co.             |2756.31      |\n",
            "|124           |Mini Gifts Distributors Ltd.|3316.18      |\n",
            "|148           |Dragon Souveniers, Ltd.     |3596.97      |\n",
            "|447           |Gift Ideas Corp.            |2366.37      |\n",
            "|323           |Down Under Souveniers, Inc  |2880.0       |\n",
            "|181           |Vitachrome Inc.             |2747.39      |\n",
            "|121           |Baane Mini Imports          |3041.78      |\n",
            "+--------------+----------------------------+-------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "---- Regional Sales ----\n",
            "+-----------+------------+\n",
            "|country    |totalRevenue|\n",
            "+-----------+------------+\n",
            "|Norway     |413682.64   |\n",
            "|Austria    |286610.64   |\n",
            "|Australia  |698111.6    |\n",
            "|country    |null        |\n",
            "|Italy      |559184.56   |\n",
            "|Philippines|408009.76   |\n",
            "|Finland    |800339.04   |\n",
            "|USA        |5205681.6   |\n",
            "|Germany    |84392.08    |\n",
            "|Denmark    |720989.36   |\n",
            "|France     |1069352.0   |\n",
            "|New Zealand|279659.52   |\n",
            "|Singapore  |661842.4    |\n",
            "+-----------+------------+\n",
            "\n",
            "---- Office Sales ----\n",
            "+----------+-------------+------------+\n",
            "|officeCode|city         |totalRevenue|\n",
            "+----------+-------------+------------+\n",
            "|officeCode|city         |1398481.9   |\n",
            "|3         |NYC          |1398481.9   |\n",
            "|5         |Tokyo        |1398481.9   |\n",
            "|6         |Sydney       |1398481.9   |\n",
            "|2         |Boston       |1398481.9   |\n",
            "|7         |London       |1398481.9   |\n",
            "|1         |San Francisco|1398481.9   |\n",
            "|4         |Paris        |1398481.9   |\n",
            "+----------+-------------+------------+\n",
            "\n",
            "---- Product Demand Summary ----\n",
            "+-----------+-------------------------------------+----------+---------------+----------------+\n",
            "|productCode|productName                          |orderCount|totalOrderedQty|averagePriceEach|\n",
            "+-----------+-------------------------------------+----------+---------------+----------------+\n",
            "|productCode|productName                          |1         |null           |null            |\n",
            "|S10_1678   |1969 Harley Davidson Ultimate Chopper|28        |1057           |85.17           |\n",
            "+-----------+-------------------------------------+----------+---------------+----------------+\n",
            "\n",
            "---- Employee Sales Summary ----\n",
            "+----------------------+-----------+----------+\n",
            "|salesRepEmployeeNumber|totalOrders|totalSales|\n",
            "+----------------------+-----------+----------+\n",
            "|1370                  |7          |28804.14  |\n",
            "|1611                  |29         |87263.95  |\n",
            "|1216                  |6          |16260.79  |\n",
            "|1612                  |14         |34957.44  |\n",
            "|salesRepEmployeeNumber|1          |null      |\n",
            "|1504                  |21         |62259.34  |\n",
            "|1286                  |28         |108044.48 |\n",
            "|1501                  |32         |100042.38 |\n",
            "|1337                  |33         |104864.86 |\n",
            "|1401                  |65         |195848.07 |\n",
            "|1166                  |31         |87450.57  |\n",
            "|1165                  |93         |293467.89 |\n",
            "|1323                  |24         |71789.41  |\n",
            "|1621                  |39         |133731.52 |\n",
            "|1188                  |27         |73697.06  |\n",
            "+----------------------+-----------+----------+\n",
            "\n",
            "---- Products Summary ----\n",
            "+-----------+-------------+-------+-------+\n",
            "|productLine|totalProducts|avgMSRP|maxMSRP|\n",
            "+-----------+-------------+-------+-------+\n",
            "|productLine|1            |null   |null   |\n",
            "|Motorcycles|1            |95.7   |95.7   |\n",
            "+-----------+-------------+-------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SySlhv0E8ob2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OyWNT5cQ8oeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DQATxbw-8og6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}